WEBVTT

0
00:00:00.009 --> 00:00:03.470
Is there, is there a principal reason why I should delete my social media?

1
00:00:03.640 --> 00:00:04.710
And if so, what is it?

2
00:00:06.639 --> 00:00:12.130
There are 21 of them is for your own good and the other is for society's good

3
00:00:12.939 --> 00:00:14.380
for your own good.

4
00:00:14.449 --> 00:00:18.159
It's because you're being subtly manipulated by

5
00:00:18.170 --> 00:00:21.459
algorithms that are watching everything you do constantly

6
00:00:21.719 --> 00:00:22.489
and then

7
00:00:22.639 --> 00:00:23.840
sending you

8
00:00:24.129 --> 00:00:28.319
changes in your media feed in your diet that are calculated

9
00:00:28.780 --> 00:00:32.779
to adjust you slightly to the liking of some unseen advertiser.

10
00:00:33.000 --> 00:00:35.049
And so if you get off that you can have a

11
00:00:35.060 --> 00:00:39.229
chance to experience a clearer view of yourself and your life.

12
00:00:39.889 --> 00:00:43.790
Uh But then the, the reason for society might be even more important.

13
00:00:44.150 --> 00:00:48.619
Society has been gradually darkened by this scheme

14
00:00:48.630 --> 00:00:51.250
in which everyone is under surveillance all the time

15
00:00:51.409 --> 00:00:52.810
and everyone is under

16
00:00:53.000 --> 00:00:56.849
this mild version of behavior modification all the time.

17
00:00:57.189 --> 00:00:58.810
It's made people

18
00:00:59.209 --> 00:01:00.740
jittery and cranky.

19
00:01:01.090 --> 00:01:05.870
It's made uh teens especially depressed, which can be quite severe.

20
00:01:06.129 --> 00:01:09.949
But it's made our politics kind of unreal and strange

21
00:01:09.959 --> 00:01:12.889
where we're not sure if elections are real anymore.

22
00:01:13.040 --> 00:01:16.690
We're not sure how much the Russians affected Brexit.

23
00:01:16.699 --> 00:01:20.150
We do know that it was a crankier affair than it might have been. Otherwise

24
00:01:20.449 --> 00:01:24.089
you say it's bad for me as an individual. Is it bad for me because I'm addicted?

25
00:01:24.099 --> 00:01:26.029
Have I become chemically hooked?

26
00:01:26.949 --> 00:01:27.889
You have

27
00:01:28.230 --> 00:01:31.790
uh the founders of the Great Silicon Valley

28
00:01:31.800 --> 00:01:35.419
spying empires like Facebook have publicly declared that

29
00:01:35.650 --> 00:01:37.099
they intentionally

30
00:01:37.239 --> 00:01:42.529
included addictive schemes in, in their designs. Now, we have to say

31
00:01:43.019 --> 00:01:46.650
this is what I would call almost a stealthy addiction.

32
00:01:47.019 --> 00:01:50.730
It's, it's a statistical addiction. What it says is

33
00:01:51.620 --> 00:01:55.529
we will get the broad population to use the services a lot.

34
00:01:55.680 --> 00:01:59.730
We'll get them hooked through a scheme of rewards and punishment.

35
00:02:00.059 --> 00:02:03.089
Uh And the, the, the, the rewards are when you're retweeted,

36
00:02:03.099 --> 00:02:06.250
the punishment is when you're treated badly by others online.

37
00:02:06.599 --> 00:02:10.410
And then within that, we'll very gradually start

38
00:02:10.619 --> 00:02:12.770
to, to leverage that to change them.

39
00:02:12.779 --> 00:02:18.059
So it's, it's this, it's this very kind of stealthy manipulation of the population.

40
00:02:18.190 --> 00:02:21.940
So it's not as dramatic as a heroin addict or a gambling addict,

41
00:02:21.949 --> 00:02:25.059
-- but it is the same principle, but who's,
-- who's doing the manipulating?

42
00:02:25.070 --> 00:02:26.210
I mean, there isn't some m

43
00:02:27.270 --> 00:02:29.490
sort of wizard of all sitting behind the screen.

44
00:02:29.500 --> 00:02:31.960
Well, this is the peculiarity of the situation.

45
00:02:32.309 --> 00:02:34.240
The people who run the tech companies like

46
00:02:34.250 --> 00:02:36.789
Google and Facebook are not doing the manipulating,

47
00:02:36.800 --> 00:02:38.119
they're doing the addicting,

48
00:02:38.589 --> 00:02:41.809
but the manipulating which rides on the back of the addicting

49
00:02:41.910 --> 00:02:45.339
is the paying customer of, of, of such a company.

50
00:02:45.509 --> 00:02:49.570
So, uh and, and many of those customers are not at all bad influences.

51
00:02:49.580 --> 00:02:54.050
They might simply be trying to promote their cars or their perfumes or whatever.

52
00:02:54.339 --> 00:02:55.449
And indeed,

53
00:02:55.839 --> 00:02:58.639
I have sympathy for them because they're concerned that

54
00:02:58.649 --> 00:03:00.380
if they don't put money into the system,

55
00:03:00.389 --> 00:03:01.679
nobody will know about them anymore.

56
00:03:01.690 --> 00:03:03.619
-- How
-- is it different to just television advertising,

57
00:03:03.899 --> 00:03:05.619
billboard advertising or anything else?

58
00:03:05.960 --> 00:03:08.820
The difference is that the constant feedback loop?

59
00:03:08.970 --> 00:03:12.300
So when you watch the television, the television isn't watching you.

60
00:03:12.309 --> 00:03:15.179
When you see the billboard, the billboard isn't seeing you.

61
00:03:15.429 --> 00:03:17.380
And vast numbers of people see the same

62
00:03:17.389 --> 00:03:19.619
thing on television and see the same billboard.

63
00:03:19.899 --> 00:03:23.500
When you use these new designs, social media search

64
00:03:23.850 --> 00:03:25.070
uh youtube,

65
00:03:25.179 --> 00:03:26.570
when you see these things,

66
00:03:26.580 --> 00:03:29.529
you're being observed constantly and algorithms are taking

67
00:03:29.539 --> 00:03:31.960
that information and changing what you see next

68
00:03:32.179 --> 00:03:34.550
and they're searching and searching and searching and,

69
00:03:34.559 --> 00:03:35.919
and they're just blind robots.

70
00:03:35.929 --> 00:03:38.149
There's no evil genius here until they find

71
00:03:38.320 --> 00:03:40.149
those patterns, those,

72
00:03:40.160 --> 00:03:43.550
those little tricks that get you and make you change your behavior

73
00:03:43.839 --> 00:03:44.919
in terms of society.

74
00:03:44.929 --> 00:03:48.529
I mean, you, you, you threw in this, you know, it's making people depressed.

75
00:03:48.649 --> 00:03:51.169
But is there any actual evidence for that?

76
00:03:51.690 --> 00:03:54.869
Yeah, unfortunately, there's a vast amount of evidence.

77
00:03:55.160 --> 00:03:57.470
There have been dozens of studies at this point,

78
00:03:57.699 --> 00:04:01.580
um including studies released by Facebook scientists. So

79
00:04:01.740 --> 00:04:04.559
this is, this is something we can call a consensus

80
00:04:04.720 --> 00:04:06.830
and, and when Facebook releases such things, they say, oh,

81
00:04:06.839 --> 00:04:08.970
but we do all these good things too that balance it.

82
00:04:08.979 --> 00:04:09.520
But there's,

83
00:04:09.630 --> 00:04:11.309
there's a general acknowledgment

84
00:04:11.570 --> 00:04:15.350
that uh depression correlates. Uh the scariest

85
00:04:15.789 --> 00:04:20.269
uh example is a correlation between rises in uh teen suicide and the,

86
00:04:20.279 --> 00:04:22.059
and the rise in use of social media.

87
00:04:22.070 --> 00:04:26.010
-- And so, yes, unfortunately, this is
-- real. But are you sure you can blame it on

88
00:04:26.359 --> 00:04:27.119
social media?

89
00:04:27.130 --> 00:04:27.339
Is it,

90
00:04:27.350 --> 00:04:30.589
is it not just those two things may have happened at the same time for other reasons?

91
00:04:30.660 --> 00:04:31.369
Well,

92
00:04:31.380 --> 00:04:33.350
here's a distinction we have to make it's very

93
00:04:33.359 --> 00:04:35.690
similar to the problem of global climate change.

94
00:04:35.700 --> 00:04:40.130
We can say statistically over the whole population. Yes, the correlation is real

95
00:04:40.529 --> 00:04:44.369
and any particular person, of course, we can't, just as we can't blame any

96
00:04:44.475 --> 00:04:47.375
particular storm on global warming. It's causality, isn't it?

97
00:04:47.475 --> 00:04:51.755
It, yeah, I mean, uh it is causality and it's,

98
00:04:51.765 --> 00:04:54.214
this is something that's very well demonstrated.

99
00:04:54.234 --> 00:04:55.084
Uh So,

100
00:04:55.214 --> 00:04:58.614
uh when the company's own scientists are publishing on,

101
00:04:58.625 --> 00:05:02.404
on this topic and come to the same agreement, I think it's time to say this is real.

102
00:05:03.549 --> 00:05:04.750
Why have you

103
00:05:06.299 --> 00:05:08.959
sort of turned on your own kind?

104
00:05:09.779 --> 00:05:13.700
I love Silicon Valley and I do not at all feel that I've turned on my own kind.

105
00:05:13.709 --> 00:05:17.149
And just to be clear, I'm very much a part of this. I, I've sold the company to Google.

106
00:05:17.160 --> 00:05:18.959
I'm not in any sense an outsider.

107
00:05:19.250 --> 00:05:22.720
I believe that what we're doing is not in our own self interest.

108
00:05:22.850 --> 00:05:23.329
Uh

109
00:05:24.410 --> 00:05:27.100
business interests are a part of society.

110
00:05:27.109 --> 00:05:29.640
If they destroy society, they destroy themselves.

111
00:05:30.109 --> 00:05:31.899
I believe it's very clear

112
00:05:32.019 --> 00:05:34.359
that we could offer all of the good things.

113
00:05:34.369 --> 00:05:34.940
And there are many,

114
00:05:34.950 --> 00:05:38.609
many good things in these services and social media in particular.

115
00:05:38.839 --> 00:05:40.940
I'm convinced we can offer them without

116
00:05:40.950 --> 00:05:43.200
this manipulation engine in the background.

117
00:05:43.470 --> 00:05:47.049
There's a world of other business plans and I think they'd be better for us.

118
00:05:47.160 --> 00:05:50.630
So III, I don't think we're being evil so much as we're being stupid

119
00:05:53.179 --> 00:05:56.980
when it comes to Facebook has Facebook made itself safe yet

120
00:05:57.989 --> 00:06:01.089
-- in terms of data harvesting and scraping and all of
-- that.

121
00:06:01.459 --> 00:06:02.739
Well, um

122
00:06:03.750 --> 00:06:06.130
Facebook's fundamental design

123
00:06:06.230 --> 00:06:08.239
is one that is uh

124
00:06:09.190 --> 00:06:14.070
it's, it's the business model is to addict you and then offer a channel to you,

125
00:06:14.079 --> 00:06:16.809
to third parties to take advantage of that to change

126
00:06:16.820 --> 00:06:18.970
you in some way without you realizing it's happening.

127
00:06:18.980 --> 00:06:20.690
I mean, that's, that's what it does.

128
00:06:21.010 --> 00:06:23.450
So I don't think any amount of tweaking

129
00:06:23.609 --> 00:06:28.209
can uh fully heal it. I think it needs a different business plan

130
00:06:28.480 --> 00:06:29.040
you, I mean,

131
00:06:29.049 --> 00:06:33.440
it's very hard to throw a barrage of rules at somebody who's

132
00:06:33.450 --> 00:06:36.459
following certain incentives and then expect them to really make a difference.

133
00:06:36.470 --> 00:06:39.299
So when Mark Zuckerberg says he's taking action and you know,

134
00:06:39.309 --> 00:06:41.339
he regrets what's happened and all the rest of it,

135
00:06:41.760 --> 00:06:46.269
you're saying he can't make his own product a safe and desirable product.

136
00:06:47.570 --> 00:06:50.920
I believe that as long as his business incentives are

137
00:06:50.929 --> 00:06:53.589
contrary to the interests of the people who use it,

138
00:06:53.600 --> 00:06:55.250
who are different from the customers.

139
00:06:55.510 --> 00:06:56.700
Uh then

140
00:06:57.029 --> 00:07:00.410
no matter how sincere he is, and I believe he's, he's sincere

141
00:07:00.540 --> 00:07:05.459
and no matter how clever he is, he can't undo that problem.

142
00:07:05.470 --> 00:07:08.950
He has to go back to the basics and change the nature of the business plan.

143
00:07:09.440 --> 00:07:13.100
And if he, if he doesn't agree with that and says, we're just gonna carry on

144
00:07:13.279 --> 00:07:18.380
how much of an how mu how mu how important is security of that data

145
00:07:18.570 --> 00:07:20.660
uh and the inability to

146
00:07:21.019 --> 00:07:24.600
repeat what has happened with Cambridge Analytica and

147
00:07:24.790 --> 00:07:28.940
-- all that kind of sort of data harvesting that went on. I
-- don't believe that this is,

148
00:07:29.350 --> 00:07:32.720
I don't believe that what happened with Cambridge Analytica is the worst of it.

149
00:07:33.049 --> 00:07:33.510
Uh

150
00:07:33.640 --> 00:07:35.890
The whole system is designed for this,

151
00:07:35.899 --> 00:07:38.739
like let's suppose that Facebook reforms itself so that the

152
00:07:38.750 --> 00:07:41.630
next Cambridge Analytica can't get access to that data.

153
00:07:41.980 --> 00:07:44.660
They can still get access to the same results

154
00:07:45.010 --> 00:07:45.829
because

155
00:07:46.040 --> 00:07:49.359
the service Facebook offers is exactly what

156
00:07:49.790 --> 00:07:49.820
C

157
00:07:49.989 --> 00:07:50.260
analytics.

158
00:07:50.559 --> 00:07:51.299
Yeah, I mean,

159
00:07:51.410 --> 00:07:56.839
this is, you know, um there are uh bad actors are,

160
00:07:56.970 --> 00:08:01.549
are, are, are able to use Facebook in ways that Facebook can't understand because

161
00:08:01.839 --> 00:08:05.630
the way the service is designed is fundamentally to be manipulative. So

162
00:08:05.910 --> 00:08:10.609
I think the data protection idea is a sincere and good idea,

163
00:08:10.619 --> 00:08:12.250
but it's certainly not adequate.

164
00:08:12.260 --> 00:08:15.779
It doesn't address the core problem, which is the manipulation engine.

165
00:08:15.790 --> 00:08:20.200
And as long as that, is there a bad actor can find a way to utilize it.

166
00:08:20.429 --> 00:08:24.119
So uh to me this, this concern about data protection

167
00:08:24.299 --> 00:08:27.769
while laudable doesn't address the core problem,

168
00:08:27.779 --> 00:08:29.649
-- do you think they're
-- all as bad as each other?

169
00:08:30.029 --> 00:08:32.558
I mean, you know, why is something like youtube

170
00:08:32.840 --> 00:08:36.440
which is basically just a way of watching video bad for you

171
00:08:37.659 --> 00:08:40.700
youtube, it's not necessarily bad for you.

172
00:08:40.710 --> 00:08:43.500
Uh Remember this is a statistical distribution.

173
00:08:43.510 --> 00:08:45.229
So for some percentage of people,

174
00:08:45.239 --> 00:08:50.190
it'll have an effect of making them uh crankier around election time and feeling

175
00:08:50.200 --> 00:08:53.219
needier around the time they might be making a purchase and so forth.

176
00:08:53.440 --> 00:08:58.400
And the way it works is that all the data Google can get on you much of which comes from

177
00:08:58.580 --> 00:09:00.190
um just your email or,

178
00:09:00.200 --> 00:09:03.539
or whatever else it might be is fed into an engine

179
00:09:03.549 --> 00:09:06.710
that compares you with other people who share some similar traits

180
00:09:07.030 --> 00:09:10.400
and youtube's ordering of videos that are presented to you

181
00:09:10.619 --> 00:09:14.840
is designed to on the one hand maximize your engagement.

182
00:09:14.849 --> 00:09:16.849
So you won't stop watching, but that's

183
00:09:17.030 --> 00:09:18.710
achieved not just by observing you,

184
00:09:18.719 --> 00:09:21.309
but by a multitude of people who are similar to you.

185
00:09:21.460 --> 00:09:23.169
And then when you do get an ad,

186
00:09:23.179 --> 00:09:26.609
it's contextualized in a way that has been shown to be effective,

187
00:09:26.820 --> 00:09:31.530
not only for you but for this whole population. So it's this giant statistical thing

188
00:09:31.919 --> 00:09:35.900
and it's bad for you because it leaches your free will. It makes you cranky.

189
00:09:35.989 --> 00:09:36.320
It,

190
00:09:36.330 --> 00:09:37.979
it makes the world a little darker

191
00:09:37.989 --> 00:09:41.140
because you're not perceiving reality clearly anymore.

192
00:09:41.150 --> 00:09:44.239
You're be, it's being, uh, manipulated, it's being,

193
00:09:44.369 --> 00:09:45.869
uh, tricked in a way.

194
00:09:46.090 --> 00:09:46.570
Uh,

195
00:09:46.679 --> 00:09:48.419
and, uh, it, uh, the,

196
00:09:48.429 --> 00:09:54.849
the people who are paying or maybe not paying just using the system to,

197
00:09:55.130 --> 00:09:59.619
uh, in a clever way to get at you are not necessarily, um, pleasant people.

198
00:09:59.630 --> 00:10:02.159
They're, they're, they're sort of the worst actors in some cases.

199
00:10:02.179 --> 00:10:05.789
But don't, don't some users think, look, I can handle advertising.

200
00:10:06.090 --> 00:10:09.119
You know, I know what I'm doing here. I'm getting a free service.

201
00:10:09.130 --> 00:10:12.549
Uh And, you know, they think they're manipulating me, but I know what I'm doing.

202
00:10:13.830 --> 00:10:15.289
The problem is that

203
00:10:15.460 --> 00:10:16.210
behaviors,

204
00:10:16.219 --> 00:10:20.929
techniques are often invisible to the person who's being manipulated and,

205
00:10:20.940 --> 00:10:22.349
and this has a long history.

206
00:10:22.359 --> 00:10:23.969
This has been done for a long time.

207
00:10:24.320 --> 00:10:24.770
Uh

208
00:10:24.909 --> 00:10:27.460
It used to be that the only way to be subjected to

209
00:10:27.469 --> 00:10:32.390
continuous observation and modification was to either be in an experiment.

210
00:10:32.590 --> 00:10:35.090
Uh You could be in the basement of a psychology building and

211
00:10:35.099 --> 00:10:38.900
have students tweaking you for their projects or you could join a cult

212
00:10:39.150 --> 00:10:41.500
or you could be in an abusive relationship.

213
00:10:41.789 --> 00:10:45.260
I mean, this has been done before and often the people who are in these

214
00:10:45.369 --> 00:10:47.450
situations do not realize it's happening to them.

215
00:10:47.460 --> 00:10:50.429
In fact, the whole point is that it's, it's sneaky. It's,

216
00:10:50.559 --> 00:10:54.469
it's a, it's a mechanical approach to manipulating people.

217
00:10:54.640 --> 00:10:57.130
And because it's, it's so algorithmic,

218
00:10:57.140 --> 00:10:59.609
it doesn't involve direct communication and people don't get

219
00:10:59.619 --> 00:11:01.700
the cues to understand what's happening with them.

220
00:11:01.719 --> 00:11:06.330
-- Why
-- do you think, um, social media has had the effects on politics that it has,

221
00:11:06.940 --> 00:11:11.309
you know, is, is it because of the way people respond to things on social media?

222
00:11:12.239 --> 00:11:13.469
Well, uh,

223
00:11:13.890 --> 00:11:17.299
I'd like to give you a slightly detailed answer as quickly as I can.

224
00:11:17.869 --> 00:11:18.849
And that is that,

225
00:11:19.000 --> 00:11:21.429
uh, in traditional behaviorism,

226
00:11:21.440 --> 00:11:24.989
you would give an animal or a person a little treat like candy or maybe

227
00:11:25.000 --> 00:11:26.669
an electric shock and you'd go back

228
00:11:26.679 --> 00:11:28.950
and forth between positive and negative feedback.

229
00:11:29.340 --> 00:11:32.109
And when researchers try to determine whether

230
00:11:32.119 --> 00:11:34.640
positivity or negativity is more powerful,

231
00:11:34.650 --> 00:11:36.950
they're roughly at parody, they're both important.

232
00:11:37.380 --> 00:11:41.640
But the difference with social media is that the algorithms that are

233
00:11:41.869 --> 00:11:46.530
are following, you respond very quickly. They're looking for the quick responses

234
00:11:46.750 --> 00:11:51.650
and the negative responses like getting startled or scared or irritated or angry

235
00:11:51.789 --> 00:11:54.469
tend to rise faster than

236
00:11:54.710 --> 00:11:58.299
the uh the positive responses like building trust

237
00:11:58.400 --> 00:12:01.150
or feeling good. Those things rise more slowly.

238
00:12:01.419 --> 00:12:05.320
So the algorithms naturally catch the negativity and amplify it

239
00:12:05.520 --> 00:12:09.630
and introduce negative people to each other. And all of this. And so what this does

240
00:12:09.739 --> 00:12:14.280
is it means that the algorithms discover there's more engagement possible

241
00:12:14.330 --> 00:12:17.619
uh say by promoting Isis and promoting the Arab Spring.

242
00:12:17.630 --> 00:12:21.169
And so ISIS gets more mileage or promoting uh the Ku

243
00:12:21.280 --> 00:12:24.799
Klux Klan than Black Lives Matter. Now, in the big picture,

244
00:12:24.914 --> 00:12:25.934
it's not true

245
00:12:26.085 --> 00:12:30.145
that negativity is more powerful but if you're doing this very rapid measurement

246
00:12:30.304 --> 00:12:34.815
of human impulses instead of accumulated human behavior, then

247
00:12:35.044 --> 00:12:36.914
it's the negativity that gets amplified.

248
00:12:36.924 --> 00:12:39.934
So you tend to have elections that are more driven by

249
00:12:40.145 --> 00:12:45.174
rancor and abuse and you tend to have outcomes that are kind of crazy.

250
00:12:45.505 --> 00:12:48.315
-- And so
-- the the effects on the media, we consume the news

251
00:12:48.684 --> 00:12:50.775
as well as also alarming because then

252
00:12:51.125 --> 00:12:53.375
it will be the news that makes people angry.

253
00:12:54.039 --> 00:12:57.219
This is the news that gets seen in the future or now

254
00:12:57.549 --> 00:12:58.559
rather than,

255
00:12:59.030 --> 00:13:03.140
you know, a, a more balanced diet of what's really going on in the world.

256
00:13:03.150 --> 00:13:03.590
Well,

257
00:13:03.599 --> 00:13:06.750
I think what goes on on a show like this is that you have

258
00:13:06.760 --> 00:13:10.559
a bit of a longer time horizon in by which you measure success.

259
00:13:10.830 --> 00:13:12.859
So you, you have to um

260
00:13:13.010 --> 00:13:15.440
impress your viewership enough to tune in,

261
00:13:15.549 --> 00:13:18.109
but this is over a process of days and weeks and months

262
00:13:18.119 --> 00:13:21.380
and years and you build up a sense of rapport with your,

263
00:13:21.390 --> 00:13:22.580
your viewership, right?

264
00:13:22.940 --> 00:13:26.599
Um If you're an algorithm that's just looking at instant responses,

265
00:13:26.609 --> 00:13:27.320
you don't get that.

266
00:13:27.330 --> 00:13:29.880
It's just like how did I engage this person?

267
00:13:30.080 --> 00:13:31.840
And it'll be, you'll,

268
00:13:31.849 --> 00:13:33.609
you'll find that engagement more often by

269
00:13:33.619 --> 00:13:35.880
irritating people than by educating them.

270
00:13:36.580 --> 00:13:39.640
And so is that how you create Trump

271
00:13:40.650 --> 00:13:40.909
uh

272
00:13:41.109 --> 00:13:41.539
or Duterte

273
00:13:42.080 --> 00:13:43.760
or you know, any of the other

274
00:13:43.950 --> 00:13:47.059
populist leaders who are doing very well at the moment, partly from the internet.

275
00:13:47.289 --> 00:13:47.789
-- I,
-- I

276
00:13:48.059 --> 00:13:53.359
have never known Trump, but I have met him a few times over a fairly long period,

277
00:13:53.369 --> 00:13:56.169
over 30 years, actually through different circumstances.

278
00:13:56.570 --> 00:13:58.159
And I will say that,

279
00:13:58.270 --> 00:14:02.409
um, well, I never would have voted for him as president and I always thought he was,

280
00:14:02.419 --> 00:14:02.679
um,

281
00:14:03.320 --> 00:14:04.179
somewhat

282
00:14:04.510 --> 00:14:08.049
untrustworthy and a bit of a, a showman and a bit of a scammer.

283
00:14:08.369 --> 00:14:13.289
He never lost himself and became so strangely insecure and

284
00:14:13.549 --> 00:14:15.539
so weirdly, um,

285
00:14:16.020 --> 00:14:16.710
um,

286
00:14:16.830 --> 00:14:20.500
irritable until he had his own addiction in this case to Twitter.

287
00:14:20.669 --> 00:14:22.780
And it's, it's really damaged him. I mean,

288
00:14:22.989 --> 00:14:25.330
I, I view Trump in a way as a victim.

289
00:14:25.710 --> 00:14:26.049
Uh

290
00:14:26.299 --> 00:14:27.289
Oh yeah, absolutely.

291
00:14:27.309 --> 00:14:30.729
His character has been really damaged by his Twitter addiction

292
00:14:30.880 --> 00:14:33.359
-- because of the reaction he gets from each tweet.
-- Yeah.

293
00:14:33.419 --> 00:14:37.890
So, you know what happens uh in addiction is the addict becomes hooked,

294
00:14:37.900 --> 00:14:41.650
not just on the good part of the addiction experience, but on the whole cycle.

295
00:14:41.659 --> 00:14:42.020
So a

296
00:14:42.210 --> 00:14:44.489
gambler is not just addicted to winning,

297
00:14:44.500 --> 00:14:46.900
but to this whole process where they mostly lose

298
00:14:47.059 --> 00:14:48.090
and in the same way,

299
00:14:48.219 --> 00:14:51.020
uh the Twitter addict or the social media addict

300
00:14:51.130 --> 00:14:55.119
becomes addicted to this engagement, which is often unpleasant

301
00:14:55.409 --> 00:14:57.640
where they're engaged in these uh you know,

302
00:14:57.650 --> 00:15:00.440
really abusive exchanges with other human beings.

303
00:15:00.450 --> 00:15:02.960
And only once in a while is that, you know, you,

304
00:15:02.969 --> 00:15:05.469
you can watch Trump like every once in a while there will be

305
00:15:05.479 --> 00:15:08.789
this tweet where somebody likes him and that's when he gets his little uh

306
00:15:08.950 --> 00:15:11.039
we call it in the trade, the dopamine hit.

307
00:15:11.309 --> 00:15:11.770
Uh uh,

308
00:15:11.969 --> 00:15:14.090
that's what it's called in Facebook, for instance,

309
00:15:14.179 --> 00:15:16.919
he gets this little dopamine hit and then he dives in for

310
00:15:16.929 --> 00:15:19.929
more negativity and things and he gets it again and you,

311
00:15:19.940 --> 00:15:21.809
you can see the addiction playing out.

312
00:15:22.219 --> 00:15:24.090
Do you think it's possible to create

313
00:15:24.309 --> 00:15:24.869
a

314
00:15:25.109 --> 00:15:25.419
do

315
00:15:25.580 --> 00:15:27.640
-- Gooding social network?
-- Yes,

316
00:15:27.900 --> 00:15:29.229
I'm absolutely positive.

317
00:15:29.349 --> 00:15:33.460
And the way to do it is to have a different business model where instead of so right now

318
00:15:33.469 --> 00:15:37.030
we've created this bizarre society that's unprecedented where if

319
00:15:37.039 --> 00:15:39.530
any two people wish to communicate over the internet,

320
00:15:39.789 --> 00:15:41.419
the only way that can happen,

321
00:15:41.429 --> 00:15:44.090
the only way it's financed is through a third party who

322
00:15:44.099 --> 00:15:47.049
believes that those two can be manipulated in a sneaky way.

323
00:15:47.059 --> 00:15:50.330
It's, it's a, it's an insane way to structure civilization

324
00:15:50.640 --> 00:15:54.330
so we can keep all the good stuff and there is good stuff on social media.

325
00:15:54.340 --> 00:15:55.349
Of course,

326
00:15:55.409 --> 00:15:57.969
we can keep all that and just throw away the

327
00:15:57.979 --> 00:16:01.640
manipulation business model and substitute in a different business model.

328
00:16:01.650 --> 00:16:04.049
And, and there are many alternatives that would be better.

329
00:16:04.299 --> 00:16:05.760
They just have to be honest.

330
00:16:05.770 --> 00:16:09.539
Uh, it could be a paid service like a Netflix where you're paying for it,

331
00:16:09.549 --> 00:16:11.159
you're the genuine customer.

332
00:16:11.289 --> 00:16:12.840
It has to keep your interest.

333
00:16:12.979 --> 00:16:14.820
It could be like a public library.

334
00:16:14.830 --> 00:16:18.960
It could become a, a public thing that is, uh, that isn't commercial at all.

335
00:16:18.969 --> 00:16:19.760
That's an option.

336
00:16:20.049 --> 00:16:20.070
Uh,

337
00:16:20.530 --> 00:16:23.869
but what we did in Silicon Valley is. We wanted it both ways.

338
00:16:23.880 --> 00:16:28.140
We wanted everything open and free, but we wanted hero entrepreneurs and hackers.

339
00:16:28.390 --> 00:16:31.340
And so the only way to get that was this advertising thing that,

340
00:16:31.349 --> 00:16:35.349
that gradually turned into the manipulation engine as the computers got faster

341
00:16:35.590 --> 00:16:38.130
and this, this weird business plan, it,

342
00:16:38.190 --> 00:16:40.320
once you can see that there are alternatives,

343
00:16:40.330 --> 00:16:43.150
you realize how strange it is and how unsustainable it is.

344
00:16:43.219 --> 00:16:44.979
This is the thing we must get rid of.

345
00:16:45.080 --> 00:16:46.770
We don't have to get rid of the smartphone.

346
00:16:46.780 --> 00:16:48.869
We don't have to get rid of the idea of social media.

347
00:16:48.880 --> 00:16:51.250
We just have to get rid of the manipulation machine

348
00:16:51.440 --> 00:16:52.630
that's in the background.

349
00:16:52.929 --> 00:16:52.940
Uh

350
00:16:53.109 --> 00:16:57.609
-- Just
-- one last thing as well. That is also obsessing parents at the moment. Um

351
00:16:57.729 --> 00:17:03.250
Screen time itself. Do you think that is a bad thing or is it just what's on the screen?

352
00:17:03.700 --> 00:17:09.439
Uh to be um frank with you? I struggle with this question because I have an 11 year old.

353
00:17:09.449 --> 00:17:10.118
And so

354
00:17:10.479 --> 00:17:15.239
I, I tend to think that manipulation time when the kids are,

355
00:17:15.250 --> 00:17:18.118
are being observed by algorithms and tweaked by them

356
00:17:18.150 --> 00:17:22.118
is vastly worse than just screen time by itself.

357
00:17:22.219 --> 00:17:22.839
So

358
00:17:23.159 --> 00:17:26.358
-- to include
-- video games and, and in the social media, you know,

359
00:17:26.368 --> 00:17:29.149
the things that are manipulating them because they are similarly addictive,

360
00:17:29.239 --> 00:17:29.548
aren't they?

361
00:17:30.129 --> 00:17:33.288
They're addictive but not manipulative typically.

362
00:17:33.298 --> 00:17:34.468
Now there I here,

363
00:17:34.479 --> 00:17:38.588
I'm not sure how evil we've become lately because there might be

364
00:17:38.598 --> 00:17:42.359
some video games that are using behavior mod techniques for pay.

365
00:17:42.369 --> 00:17:43.788
That's conceivable.

366
00:17:43.798 --> 00:17:47.909
I can see how that could happen if you're thinking about it out there. Don't do it. OK.

367
00:17:47.918 --> 00:17:49.308
Find something better to do.

368
00:17:49.599 --> 00:17:54.420
But the, the mainstream video games are not doing that, they are addictive.

369
00:17:54.430 --> 00:17:55.839
So there are plenty of things that are

370
00:17:55.849 --> 00:17:58.900
addictive that aren't leveraging that for manipulation.

371
00:17:58.910 --> 00:18:01.619
So these are two different stages. What do you think of Fortnite?

372
00:18:02.660 --> 00:18:05.869
-- I have not played
-- it. You know, I haven't played it because Fort Knox is exactly that.

373
00:18:05.880 --> 00:18:07.239
It's getting people to pay

374
00:18:07.449 --> 00:18:08.280
for

375
00:18:08.390 --> 00:18:09.530
things within the game.

376
00:18:09.819 --> 00:18:10.109
But see,

377
00:18:10.119 --> 00:18:14.680
the thing is getting them to pay is still not manipulating them for a third party

378
00:18:15.030 --> 00:18:19.479
that's getting them to buy stuff. I mean, Amazon does that to get you to buy stuff.

379
00:18:19.489 --> 00:18:20.880
Uh All kinds of people do that,

380
00:18:21.130 --> 00:18:21.670
that,

381
00:18:21.780 --> 00:18:23.560
that might be annoying.

382
00:18:23.569 --> 00:18:27.380
You might object to it, especially if you feel your kids are wasting money,

383
00:18:27.390 --> 00:18:28.260
you might object to it.

384
00:18:28.270 --> 00:18:30.400
You might feel it's not an ideal

385
00:18:30.650 --> 00:18:33.660
um example of human behavior and character and

386
00:18:33.670 --> 00:18:35.150
maybe there could be a better business,

387
00:18:35.160 --> 00:18:38.869
whatever there, but it's not directly manipulating,

388
00:18:38.880 --> 00:18:41.030
you say to influence an election.

389
00:18:41.290 --> 00:18:44.660
It's not trying to change your behavior out in the larger world.

390
00:18:44.670 --> 00:18:46.069
And, and that's the thing

391
00:18:46.189 --> 00:18:49.349
that's really tragic about designs like Facebook and Google,

392
00:18:49.359 --> 00:18:51.430
they are succeeding in doing that.

393
00:18:51.530 --> 00:18:52.800
-- But
-- your advice

394
00:18:52.920 --> 00:18:55.079
tonight to everyone watching this

395
00:18:55.229 --> 00:18:57.180
is delete all your accounts.

396
00:18:58.099 --> 00:19:02.319
I would like to make two very quick pitches on that account.

397
00:19:02.369 --> 00:19:07.109
One, if you're a young person and you've only lived with social media,

398
00:19:07.420 --> 00:19:10.579
your first duty is to yourself. You have to know yourself.

399
00:19:10.589 --> 00:19:14.260
You should experience travel, you should experience challenge to yourself.

400
00:19:14.270 --> 00:19:17.000
You need to know yourself and you can't know yourself

401
00:19:17.270 --> 00:19:18.599
without perspective.

402
00:19:18.810 --> 00:19:22.560
So at least give it six months without social media and really quit him.

403
00:19:22.569 --> 00:19:25.479
Don't like quit Facebook with keep another

404
00:19:25.489 --> 00:19:27.319
Facebook thing like whatsapp because then it's,

405
00:19:27.329 --> 00:19:29.180
it'll still be spying and manipulating,

406
00:19:29.319 --> 00:19:31.920
get rid of the whole thing for six months and know yourself.

407
00:19:31.930 --> 00:19:35.560
And then you can decide, I can't tell you what's right. You have to decide,

408
00:19:35.810 --> 00:19:37.630
but you can't until you know yourself.

409
00:19:37.979 --> 00:19:40.109
And then for the rest of society, I'd say

410
00:19:40.319 --> 00:19:44.369
as long as we can have some small percentage of people who are off it,

411
00:19:44.380 --> 00:19:47.500
then the society can have voices to give perspective.

412
00:19:47.510 --> 00:19:50.030
If everybody is universally part of this thing,

413
00:19:50.189 --> 00:19:53.469
we cannot have perspective. We cannot have a real conversation

414
00:19:53.739 --> 00:19:55.270
and it's too lonely right now.

415
00:19:55.280 --> 00:19:55.719
You know,

416
00:19:55.729 --> 00:19:59.000
we need more people who are just outside of

417
00:19:59.010 --> 00:20:01.609
that loop who are thinking without the manipulation.

418
00:20:01.619 --> 00:20:04.750
And I think we'll find it extraordinarily valuable to have them.

419
00:20:05.869 --> 00:20:07.410
Are you just a new age hippie?

420
00:20:08.109 --> 00:20:08.130
I

421
00:20:08.300 --> 00:20:08.810
mean,

422
00:20:09.510 --> 00:20:12.170
have you just been through the mill and kind of worked out?

423
00:20:12.180 --> 00:20:14.170
I wanna check out of all this and let's just,

424
00:20:14.469 --> 00:20:15.449
-- let's just
-- stop.

425
00:20:16.410 --> 00:20:18.910
Um Do I seem new age to you.

426
00:20:19.050 --> 00:20:19.650
I don't know.

427
00:20:19.660 --> 00:20:21.599
I mean, you know, I mean, I, here,

428
00:20:21.609 --> 00:20:24.310
here's what I'll tell you the bind you put me in

429
00:20:24.319 --> 00:20:27.369
is that I'd be happy to trash the new age.

430
00:20:27.380 --> 00:20:31.040
And, uh, and demonstrate that I'm not part of that manner of thinking.

431
00:20:31.050 --> 00:20:34.510
I'm certainly not. I think, I hope I've come across as a non utopian.

432
00:20:34.800 --> 00:20:35.869
Um, but, uh,

433
00:20:35.880 --> 00:20:38.469
the problem is many of my friends in California are quite

434
00:20:38.479 --> 00:20:41.180
new age so I want to be kind to them.