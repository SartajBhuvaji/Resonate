{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample meeting transcripts\n",
    "meeting_transcripts = pd.read_csv(\"test.csv\")[\"text\"]\n",
    "\n",
    "# Preprocess the text (you may need more advanced preprocessing based on your data)\n",
    "# Here, we are using a simple approach without stemming or lemmatization\n",
    "processed_transcripts = [\" \".join(text.split()) for text in meeting_transcripts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF Vectorization\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(processed_transcripts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mini-Batch K-Means Clustering\n",
    "num_clusters = 3  # You can adjust this based on your data or use techniques to find optimal clusters\n",
    "kmeans = MiniBatchKMeans(n_clusters=num_clusters, random_state=42)\n",
    "clusters = kmeans.fit_predict(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the cluster labels to the original data\n",
    "data = {\"Transcript\": meeting_transcripts, \"Cluster\": clusters}\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What was discussed in the project meeting?\n",
      "Most relevant cluster: 0\n",
      "Transcripts in the most relevant cluster:\n",
      "0                             Yeah, the universal one.\n",
      "2         But, but 25 you need a lot of neat features.\n",
      "3    Yeah. Yeah. Uh cause I mean, what â‚¬25 thats ab...\n",
      "Name: Transcript, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\GitHub_Repos\\SartajBhuvaji\\Resonate\\Streamlit_App\\.venv\\Lib\\site-packages\\sklearn\\decomposition\\_truncated_svd.py:273: RuntimeWarning: invalid value encountered in divide\n",
      "  self.explained_variance_ratio_ = exp_var / full_var\n"
     ]
    }
   ],
   "source": [
    "# Example of querying using natural language\n",
    "user_query = \"What was discussed in the project meeting?\"\n",
    "\n",
    "# Remove TruncatedSVD and use the original TF-IDF matrix for the query vector\n",
    "query_vector = tfidf_vectorizer.transform([user_query])\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "# Transform query vector using TruncatedSVD (LSA) for dimensionality reduction\n",
    "# Reduce the number of components in TruncatedSVD\n",
    "\n",
    "# Normalize TF-IDF matrix\n",
    "tfidf_matrix_normalized = normalize(tfidf_matrix)\n",
    "\n",
    "# Apply TruncatedSVD\n",
    "lsa = TruncatedSVD(n_components=min(100, tfidf_matrix.shape[1]), random_state=42)\n",
    "query_vector_lsa = lsa.fit_transform(query_vector)\n",
    "\n",
    "# Calculate cosine similarity between query and cluster centroids\n",
    "cluster_centers = kmeans.cluster_centers_\n",
    "# similarities = linear_kernel(query_vector_lsa, cluster_centers)\n",
    "\n",
    "# Calculate cosine similarity between query and cluster centroids\n",
    "similarities = linear_kernel(query_vector, cluster_centers)\n",
    "\n",
    "# Get the most similar cluster\n",
    "most_similar_cluster = similarities.argmax()\n",
    "\n",
    "# Retrieve transcripts from the most similar cluster\n",
    "relevant_transcripts = df[df[\"Cluster\"] == most_similar_cluster][\"Transcript\"]\n",
    "\n",
    "# Display results\n",
    "print(\"Query:\", user_query)\n",
    "print(\"Most relevant cluster:\", most_similar_cluster)\n",
    "print(\"Transcripts in the most relevant cluster:\")\n",
    "print(relevant_transcripts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Assume new_transcript is the new meeting transcript\n",
    "new_transcript = \"Discussion on the upcoming project deadline.\"\n",
    "\n",
    "# Preprocess the new transcript\n",
    "processed_new_transcript = \" \".join(new_transcript.split())\n",
    "\n",
    "# Vectorize the new transcript using the same TF-IDF vectorizer\n",
    "new_transcript_vector = tfidf_vectorizer.transform([processed_new_transcript])\n",
    "\n",
    "# Transform the new transcript vector using TruncatedSVD if needed\n",
    "new_transcript_vector_lsa = lsa.transform(new_transcript_vector)\n",
    "\n",
    "# Calculate cosine similarity between the new transcript and existing cluster centroids\n",
    "similarities_to_clusters = cosine_similarity(new_transcript_vector_lsa, cluster_centers)\n",
    "\n",
    "# Set a similarity threshold below which a new cluster will be created\n",
    "similarity_threshold = 0.7\n",
    "\n",
    "# Find the cluster with the highest similarity\n",
    "most_similar_cluster_index = similarities_to_clusters.argmax()\n",
    "highest_similarity = similarities_to_clusters[0, most_similar_cluster_index]\n",
    "\n",
    "# Check if the highest similarity is below the threshold\n",
    "if highest_similarity < similarity_threshold:\n",
    "    # Create a new cluster\n",
    "    new_cluster_index = len(cluster_centers)\n",
    "    cluster_centers = np.vstack([cluster_centers, new_transcript_vector_lsa])\n",
    "    df = df.append({'Transcript': new_transcript, 'Cluster': new_cluster_index}, ignore_index=True)\n",
    "    print(f\"New cluster created for the transcript: {new_transcript}\")\n",
    "else:\n",
    "    # Add the new transcript to the most similar existing cluster\n",
    "    df = df.append({'Transcript': new_transcript, 'Cluster': most_similar_cluster_index}, ignore_index=True)\n",
    "    print(f\"Added to existing cluster: {most_similar_cluster_index}\")\n",
    "\n",
    "# Display updated clusters\n",
    "print(\"Updated Clusters:\")\n",
    "print(df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
