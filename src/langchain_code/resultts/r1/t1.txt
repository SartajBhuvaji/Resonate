
INDEX_NAME = 'resonate-meeting-index' #'langchain-retrieval-transcript'
PINECONE_VECTOR_DIMENSION = 3072 #1536
PINECONE_UPSERT_BATCH_LIMIT = 90
PINECONE_TOP_K_RESULTS = 3
DELTA = 5
CLOUD_PROVIDER = 'aws'
REGION = 'us-west-2'
METRIC = 'cosine'

EMBEDDING = 'OpenAI'
EMBEDDING_MODEL = 'text-embedding-3-large' #'text-embedding-ada-002'

NAMESPACE = 'default_namespace'
master_json_file = 'master_meeting_details'

LLM_MODEL = 'gpt-3.5-turbo'
LLM_TEMPERATURE = 0.0
CONV_BUFFER_MEMORY_WINDOW = 2
LLM_SUMMARY_MAX_TOKEN_LIMIT = 650

class LangChain:
    def __init__(self):
        self.pinecone_obj = PineconeServerless()
        self.llm=ChatOpenAI(temperature=LLM_TEMPERATURE, model_name=LLM_MODEL, streaming=False)
        #self.llm=ChatCohere(model='command', temperature=0)
        #self.conversation_bufw = ConversationChain(llm=self.llm, memory=ConversationBufferWindowMemory(k=CONV_BUFFER_MEMORY_WINDOW))
        self.conversation_bufw = ConversationChain(llm=self.llm, memory=ConversationSummaryBufferMemory(llm=self.llm, max_token_limit=LLM_SUMMARY_MAX_TOKEN_LIMIT))
        #self.conversation_bufw = ConversationChain(llm=self.llm, memory=ConversationSummaryMemory(llm=self.llm))
        self.df = pd.DataFrame(columns=['Query', 'LLM Input', 'History', 
                                        'LLM Response', 'Tokens Used',
                                        'Prompt Tokens','Completion Tokens', 
                                        'Total Cost (USD)'])

    def prompt(self, query, context):
        system_template = SystemMessagePromptTemplate.from_template(
            'You are a helpful assistant.'
            'You are provided with a context below. You are expected to answer the user query based on the context below.'
            'The context provided is a part of transcript of a meeting, in the format:'
            'Conversations in meeting: <meeting_title>'
            'Start Time - Speaker: Text \n'

            'You will respond using the context below only. If you cannot find an answer from the below context, you can ask for more information.'
            'You answers should be concise and relevant to the context.'
            'You can mention the meeting_title in your response if you want to refer to the meeting.'
            'You are not allowed to talk about anything else other than the context below.'
            'You cannot use any external information other than the context below.'
            'No need to greet or say goodbye. Just answer the user query based on the context below.'
            'You can also skip mentioning phrases such as : Based on the context provided. Instead simply answer the user query based on the context below.\n\n'
            'Context:\n'
            '{context}'
        )
        # system_template = SystemMessagePromptTemplate.from_template(
        #     'You are a helpful assistant.'
        #     'You will answer the user query based on the context below.'
        #     'You are also provided with the chat history of the user query and the response. You can use this information to answer the user query as well'
        #     'Context: \n'
        #     '{context}'
        # )

        human_template = HumanMessagePromptTemplate.from_template(' \nUser Query: {input}')
        chat_prompt = ChatPromptTemplate.from_messages([system_template, human_template])
        
        chat_prompt_value = chat_prompt.format_prompt(
            context = context,
            input = query
        )
        #print(chat_prompt_value)
        return chat_prompt_value.to_messages()


    def query_chatbot(self, query, context):
        self.messages = self.prompt(query, context)
        #resp = self.conversation_bufw(self.messages)
        resp, callback = self.count_tokens(self.conversation_bufw, self.messages)
        # append resp, callback to df
        #print("Resp: ", resp)
        #print("Callback: ", callback)

    
        self.df = pd.concat([self.df, pd.DataFrame({
            'Query': query,
            'LLM Input': str(resp['input']), 
            'History': str(resp['history']), 
            'LLM Response': str(resp['response']), 
            'Tokens Used': callback['Tokens Used'],
            'Prompt Tokens': callback['Prompt Tokens'],
            'Completion Tokens': callback['Completion Tokens'],
            'Total Cost (USD)': str(callback['Total Cost (USD)']).replace('$', '')
             }, 
            index = [0])], ignore_index=True)

        print("Tokens Used: ", callback['Tokens Used'])
        return resp
        #return resp['response']
    
    def parse_conversations(self, conversations) -> str:
        data = []
        for cluster_id, cluster_df in conversations.items():
            with open(f'../../bin/data/default_namespace/{cluster_id}.json') as f:
                meeting_data = json.load(f)
                meeting_title = meeting_data['meeting_title']
                data.append(f"Conversations in meeting '{meeting_title}':")
                for i, row in cluster_df.iterrows():
                    data.append(f"{row['start_time']} - {row['speaker']}: {row['text']}")
                data.append("\n\n")
        data = '\n'.join(data)
        return data

    def clear_conversational_memory(self):
        self.conversation_bufw.memory.clear()

    def chat(self, query, in_filter: list[str]=[], complete_db_flag:bool = True):
        if 'summary' in query:
            pass
        self.pinecone_obj.query_pinecone(query, in_filter, complete_db_flag)
        conversation = self.pinecone_obj.query_delta_conversations()
        context = self.parse_conversations(conversation)
        #print(context)
        try:
            response = self.query_chatbot(query, context)
        except Exception as e:
            print(f'Error: {e}')
            response = "Oops! you have exhausted the token limit, clearing the conversational memory. Please try again."   
            self.clear_conversational_memory() 
        return response
    
    def count_tokens(self, chain, query):
        with get_openai_callback() as callback:
            response = chain(query)
            #print(f'Call Back:  {callback}')
            print(f'Spent a total of {callback.total_tokens} tokens')
            callback = str(callback)
            lines = callback.split('\n')
            data = {}
            for line in lines:
                parts = line.split(':')
                if len(parts) == 2:
                    key = parts[0].strip()
                    value = parts[1].strip()
                    data[key] = str(value)
            #print(data)       
            return response, data
    