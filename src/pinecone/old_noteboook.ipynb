{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sbhuv\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pinecone\\data\\index.py:1: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import dotenv\n",
    "import json\n",
    "import datetime\n",
    "import os\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "import time\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "# PENDING : Move these to a config file\n",
    "INDEX_NAME = 'langchain-retrieval-transcript'\n",
    "PINECONE_VECTOR_DIMENSION = 1536\n",
    "PINECONE_UPSERT_BATCH_LIMIT = 90\n",
    "PINECONE_TOP_K_RESULTS = 2\n",
    "DELTA = 2\n",
    "CLOUD_PROVIDER = 'aws'\n",
    "REGION = 'us-west-2'\n",
    "METRIC = 'cosine'\n",
    "\n",
    "EMBEDDING = 'OpenAI'\n",
    "EMBEDDING_MODEL = 'text-embedding-ada-002'\n",
    "\n",
    "class PineconeServerless:\n",
    "    def __init__(self, namespace: str = 'default_namespace') -> None:\n",
    "        PINECONE_API_KEY = os.getenv('PINECONE_SERVERLESS_API_KEY') or 'PINECONE_SERVERLESS_API_KEY'\n",
    "        self.OPENAI_API_KEY = os.getenv('OPENAI_API_KEY') or 'OPENAI_API_KEY'\n",
    "        self.index_name = INDEX_NAME\n",
    "        self.namespace = namespace\n",
    "        self.pinecone = Pinecone(api_key=PINECONE_API_KEY)\n",
    "        self.base_data_path = os.path.join(os.getcwd(), '../../','bin/data/', INDEX_NAME)\n",
    "\n",
    "\n",
    "    def check_index_already_exists(self) -> bool:\n",
    "        return self.index_name in self.pinecone.list_indexes()\n",
    "\n",
    "\n",
    "    def _get_index(self):\n",
    "        return self.pinecone.Index(self.index_name)\n",
    "    \n",
    "\n",
    "    def _create_index(self, INDEX_NAME: str) -> None:\n",
    "        try:\n",
    "            self.pinecone.create_index(\n",
    "                name=INDEX_NAME,\n",
    "                metric=METRIC,\n",
    "                dimension=PINECONE_VECTOR_DIMENSION,\n",
    "            \n",
    "                spec=ServerlessSpec(\n",
    "                    cloud=CLOUD_PROVIDER, \n",
    "                    region=REGION,\n",
    "                    # pod_type=\"p1.x1\",\n",
    "                ) \n",
    "            )    \n",
    "\n",
    "            while not self.pinecone.describe_index(self.index_name).status['ready']:\n",
    "                time.sleep(5)\n",
    "\n",
    "        except Exception as e:\n",
    "            print('Index creation failed: ', e)      \n",
    "\n",
    "\n",
    "    def describe_index_stats(self) -> dict:\n",
    "        try:\n",
    "            index = self._get_index()\n",
    "            return index.describe_index_stats()\n",
    "        except Exception as e:\n",
    "            print('Index does not exist: ', e)\n",
    "            return {}\n",
    "\n",
    "    \n",
    "    def _delete_index(self, index_name: str) -> None:\n",
    "        try:\n",
    "            self.pinecone.delete_index(index_name)\n",
    "        except Exception as e:\n",
    "            print('Index does not exist: ', e)\n",
    "\n",
    "\n",
    "    def _set_new_meeting_json(self, namespace: str, last_conversation_no: str, meeting_video_file: bool,meeting_members: list[str]) -> dict:\n",
    "        data = {\n",
    "                \"index\": INDEX_NAME,\n",
    "                \"namespace\": namespace,\n",
    "                \"last_meeting_no\": 1,\n",
    "                \"last_conversation_no\": last_conversation_no,\n",
    "                \"unique_meeting_members\": meeting_members,\n",
    "                \"meetings\": [\n",
    "                    {\n",
    "                        \"meeting_no\": 1,\n",
    "                        \"meeting_last_conversation_no\": last_conversation_no,\n",
    "                        \"meeting_date\": datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                        \"meeting_video_file\": meeting_video_file,\n",
    "                        \"meeting_members\": meeting_members,\n",
    "                    },\n",
    "                ]\n",
    "            }\n",
    "        return data\n",
    "    \n",
    "\n",
    "    def _append_meeting_details(self,meeting_details_file: str, last_meeting_no: int, last_conversation_no: int, meeting_video_file: bool, meeting_members: list[str]) -> dict:\n",
    "        with open(meeting_details_file, 'r') as f:\n",
    "            data = json.load(f)\n",
    "            data['last_meeting_no'] = last_meeting_no\n",
    "            data['last_conversation_no'] = last_conversation_no \n",
    "            unique_meeting_members = set(data.get('unique_meeting_members', []))\n",
    "            unique_meeting_members.update(meeting_members)\n",
    "            data['unique_meeting_members'] = list(unique_meeting_members)\n",
    "\n",
    "            data['meetings'].append(\n",
    "                {\n",
    "                    \"meeting_no\": last_meeting_no,\n",
    "                    \"meeting_last_conversation_no\": last_conversation_no,\n",
    "                    \"meeting_date\": datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                    \"meeting_video_file\": meeting_video_file,\n",
    "                    \"meeting_members\": meeting_members,\n",
    "                }\n",
    "            )\n",
    "            return data    \n",
    "\n",
    "\n",
    "    def _get_meeting_details(self, namespace: str) -> tuple[int, int]:\n",
    "        meeting_details_file = os.path.join(self.base_data_path, f'{namespace}.json')  \n",
    "        if not os.path.exists(meeting_details_file):\n",
    "           print('Namespace does not exist in JSON Store')\n",
    "           return 0, 0\n",
    "        \n",
    "        with open(meeting_details_file, 'r') as f:\n",
    "            data = json.load(f)\n",
    "            return data['last_meeting_no'], data['last_conversation_no']\n",
    "        \n",
    "\n",
    "    def _set_meeting_details(self, namespace: str, last_meeting_no: int, \n",
    "                             last_conversation_no: int, meeting_video_file: bool, meeting_members: list[str] ) -> None:\n",
    "        # PENDING : Update the meeting details in the Firebase database\n",
    "\n",
    "        if not os.path.exists(self.base_data_path):\n",
    "            os.makedirs(self.base_data_path)\n",
    "\n",
    "        meeting_details_file = os.path.join(self.base_data_path, f'{namespace}.json')  \n",
    "\n",
    "        if not os.path.exists(meeting_details_file):\n",
    "            data = self._set_new_meeting_json(namespace, last_conversation_no,meeting_video_file, meeting_members)\n",
    "        else:\n",
    "            data = self._append_meeting_details(meeting_details_file, last_meeting_no, last_conversation_no, meeting_video_file, meeting_members)\n",
    "\n",
    "        with open(meeting_details_file, 'w') as f:\n",
    "            json.dump(data, f, indent=4)\n",
    "\n",
    "    def _get_meeting_members(self, transcript: pd.DataFrame) -> list[str]:\n",
    "        return list(transcript['speaker_label'].unique())\n",
    "    \n",
    "        \n",
    "    def _get_vector_embedder(self, EMBEDDING: str = 'OpenAI'):\n",
    "        if EMBEDDING == 'OpenAI':\n",
    "            return OpenAIEmbeddings(\n",
    "                model=EMBEDDING_MODEL,\n",
    "                openai_api_key=self.OPENAI_API_KEY)\n",
    "        \n",
    "    \n",
    "    def get_entire_namespace_data(self, namespace:str) -> pd.DataFrame: \n",
    "        _ , last_conversation_no = self._get_meeting_details(namespace)\n",
    "        all_conversations = [str(i) for i in range(1, last_conversation_no+1)]\n",
    "\n",
    "        index = self._get_index()\n",
    "        fetch_response = index.fetch(ids=all_conversations, namespace=self.namespace)\n",
    "        conversation = {}\n",
    "        conversation[0] = fetch_response\n",
    "        \n",
    "        entire_namespace_data = self._parse_fetch_conversations(conversation)\n",
    "        entire_namespace_data['namespace'] = namespace\n",
    "        entire_namespace_data.drop(columns=['primary_id'], inplace=True)\n",
    "        entire_namespace_data = entire_namespace_data.sort_values(by=['id'])\n",
    "        return entire_namespace_data\n",
    "       \n",
    "    def pinecone_upsert(self, transcript: pd.DataFrame, meeting_video_file: bool=False) -> None:\n",
    "        texts = []\n",
    "        metadatas = []\n",
    "        \n",
    "        meeting_no, last_conversation_no = self._get_meeting_details(self.namespace) \n",
    "        meeting_no += 1\n",
    "        meeting_members = self._get_meeting_members(transcript) \n",
    "        embed = self._get_vector_embedder(EMBEDDING)\n",
    "        index = self._get_index()\n",
    "\n",
    "        for _ , record in transcript.iterrows():\n",
    "            metadata = {\n",
    "                'speaker': record['speaker_label'],\n",
    "                'start_time': round(record['start_time'], 4), # fix a time format\n",
    "                'meeting_no': meeting_no,\n",
    "                'text': record['text'], \n",
    "            }\n",
    "\n",
    "            texts.append(record['text']) \n",
    "            metadatas.append(metadata)\n",
    "\n",
    "            if len(texts) >= PINECONE_UPSERT_BATCH_LIMIT:\n",
    "                ids = list(map(lambda i: str(i+1), range(last_conversation_no, last_conversation_no + len(texts))))\n",
    "                last_conversation_no += len(texts)\n",
    "                ids = [meeting_no] * len(texts)\n",
    "                embeds = embed.embed_documents(texts)\n",
    "                try:\n",
    "                    index.upsert(vectors=zip(ids, embeds, metadatas), namespace=self.namespace)\n",
    "                except Exception as e:\n",
    "                    print('Error upserting into Pinecone: ', e)    \n",
    "                texts = []\n",
    "                metadatas = []\n",
    "\n",
    "        if len(texts) > 0:\n",
    "            ids = list(map(lambda i: str(i+1), range(last_conversation_no, last_conversation_no + len(texts))))\n",
    "            last_conversation_no += len(texts)\n",
    "            embeds = embed.embed_documents(texts)\n",
    "            try:\n",
    "                index.upsert(vectors=zip(ids, embeds, metadatas), namespace=self.namespace)\n",
    "            except Exception as e:\n",
    "                print('Error upserting into Pinecone: ', e)\n",
    "\n",
    "        self._set_meeting_details(self.namespace, meeting_no, last_conversation_no, meeting_video_file, meeting_members)     \n",
    "\n",
    "\n",
    "    def query_pinecone(self, query: str, namespace: str) -> list:\n",
    "        try:\n",
    "            index = self._get_index()\n",
    "            embed = self._get_vector_embedder(EMBEDDING)\n",
    "            self.response = index.query(\n",
    "                vector= embed.embed_documents([query])[0],\n",
    "                namespace=namespace, \n",
    "                top_k=PINECONE_TOP_K_RESULTS,\n",
    "                include_metadata=True,\n",
    "                # filter={\"meeting_no\": {\"$in\":[1, 2]}},\n",
    "            )\n",
    "            return self.response\n",
    "        except Exception as e:\n",
    "            print('Error querying Pinecone: ', e)\n",
    "        return []\n",
    "        \n",
    "\n",
    "    def _extract_id_from_response(self, response: list) -> list[int]:\n",
    "        return list(int(match['id']) for match in response['matches'])\n",
    "\n",
    "    def _parse_query_for_namespace(self, namespace:str, namespace_response) -> pd.DataFrame:\n",
    "        parsed_namespace_response = pd.DataFrame(columns=['namespace', 'id', 'meeting_no', 'speaker', 'start_time', 'text', 'score'])\n",
    "\n",
    "        for match in namespace_response['matches']:\n",
    "                namespace = namespace\n",
    "                metadata = match['metadata']\n",
    "                id_ = int(match['id'])\n",
    "                score = match['score']\n",
    "\n",
    "                meeting_no = metadata['meeting_no']\n",
    "                start_time = metadata['start_time']\n",
    "                speaker = metadata['speaker']\n",
    "                text = metadata['text']\n",
    "\n",
    "                data = {'namespace': namespace,'id': id_, 'meeting_no': meeting_no, 'speaker': speaker,\n",
    "                    'start_time': start_time, 'text': text, 'score': score}\n",
    "                parsed_namespace_response = pd.concat([parsed_namespace_response, pd.DataFrame(data, index=[0])], ignore_index=True)     \n",
    "\n",
    "        return parsed_namespace_response\n",
    "   \n",
    "\n",
    "    def query_every_namespace(self, query:str) -> pd.DataFrame: \n",
    "        index_info = self.describe_index_stats()\n",
    "        all_namespaces = list(index_info['namespaces'].keys())\n",
    "        every_namespace_response = pd.DataFrame(columns=['namespace', 'id', 'meeting_no', 'speaker', 'start_time', 'text'])\n",
    "        for namespace in all_namespaces:\n",
    "            try:\n",
    "                namespace_response = self.query_pinecone(query, namespace=namespace)\n",
    "                parsed_namespace_response = self._parse_query_for_namespace(namespace, namespace_response)\n",
    "                every_namespace_response =  pd.concat([every_namespace_response, parsed_namespace_response], ignore_index=True)\n",
    "            except Exception as e:\n",
    "                print('Error querying Pinecone namespace: ', namespace, 'Error:' , e)\n",
    "                continue\n",
    "            \n",
    "        return every_namespace_response\n",
    "\n",
    "\n",
    "    def query_delta_conversations_all_namespaces(self, query: str) -> pd.DataFrame: ##\n",
    "        every_namespace_response = self.query_every_namespace(query)\n",
    "        all_namespaces = list(every_namespace_response['namespace'].unique())\n",
    "\n",
    "        all_namespaces_delta_coversations = pd.DataFrame(columns=['namespace', 'primary_id', 'id', 'meeting_no', 'speaker', 'start_time', 'text'])\n",
    "        for namespace in all_namespaces:\n",
    "            self.namespace = namespace\n",
    "            query_delta_conversations_parsed = self.query_delta_conversations()\n",
    "            query_delta_conversations_parsed['namespace'] = namespace  \n",
    "            all_namespaces_delta_coversations = pd.concat([all_namespaces_delta_coversations, query_delta_conversations_parsed], ignore_index=True)\n",
    "\n",
    "        return all_namespaces_delta_coversations\n",
    "\n",
    "\n",
    "    def query_delta_conversations(self) -> pd.DataFrame:\n",
    "        ids = self._extract_id_from_response(self.response)\n",
    "        _, last_conversation_no = self._get_meeting_details(self.namespace)\n",
    "        index = self._get_index()\n",
    "        conversation = {}\n",
    "\n",
    "        for id in ids: \n",
    "            left = id - DELTA if id - DELTA > 0 else 1\n",
    "            right = id + DELTA if id + DELTA <= last_conversation_no else last_conversation_no\n",
    "            window = [str(i) for i in range(left, right + 1)]    \n",
    "            try:\n",
    "                fetch_response = index.fetch(ids=window, namespace=self.namespace)\n",
    "                conversation[id] = fetch_response\n",
    "            except Exception as e:\n",
    "                print('Error fetching from Pinecone for id:', id, \"Error:\", e)\n",
    "                continue\n",
    "\n",
    "        print('conversation length: ', len(conversation))\n",
    "        return self._parse_fetch_conversations(conversation)\n",
    "\n",
    "\n",
    "    def _parse_fetch_conversations(self, conversation) -> pd.DataFrame:  \n",
    "      \n",
    "        data_rows = []\n",
    "        for primary_hit_id, primary_hit_data in conversation.items():\n",
    "            for _ , vector_data in primary_hit_data['vectors'].items():\n",
    "                id = vector_data['id']\n",
    "                meeting_no = vector_data['metadata']['meeting_no']\n",
    "                speaker = vector_data['metadata']['speaker']\n",
    "                start_time = vector_data['metadata']['start_time']\n",
    "                text = vector_data['metadata']['text']\n",
    "                \n",
    "                data_rows.append((primary_hit_id, id, meeting_no, speaker, start_time, text))\n",
    "\n",
    "        columns = ['primary_id', 'id', 'meeting_no', 'speaker', 'start_time', 'text']\n",
    "        delta_conversation_df = pd.DataFrame(data_rows, columns=columns)\n",
    "        delta_conversation_df = delta_conversation_df.sort_values(by=['id'])\n",
    "        print('LENGTH delta_conversation_df: ', len(delta_conversation_df), \"namespace: \", self.namespace)\n",
    "        return delta_conversation_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "namespace1 = 'namespace1'\n",
    "obj1 = PineconeServerless(namespace=namespace1)\n",
    "\n",
    "# namespace2 = 'namespace2'\n",
    "# obj2 = PineconeServerless(namespace=namespace2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obj1._delete_index(INDEX_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj1._create_index(INDEX_NAME)\n",
    "## obj2._create_index(INDEX_NAME)  # No need as both obj1 and obj2 are pointing to the same index\n",
    "\n",
    "print(obj1.describe_index_stats())\n",
    "## print(obj2.describe_index_stats()) # No need as both obj1 and obj2 are pointing to the same index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 3):\n",
    "    print(i)\n",
    "    transcript = pd.read_csv(f'transcript_{i}.csv')\n",
    "    transcript.dropna(inplace=True)\n",
    "    obj1.pinecone_upsert(transcript)\n",
    "    time.sleep(5)\n",
    "\n",
    "for i in range(3, 5):\n",
    "    print(i)\n",
    "    transcript = pd.read_csv(f'transcript_{i}.csv')\n",
    "    transcript.dropna(inplace=True)\n",
    "    obj2.pinecone_upsert(transcript)\n",
    "    time.sleep(5)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj1.describe_index_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response1 = obj1.query_pinecone('What was talked about CIN agent?', namespace1)\n",
    "print(response1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(obj1._extract_id_from_response(response1))\n",
    "# print(obj2._extract_id_from_response(response2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation1_df = obj1.query_delta_conversations()\n",
    "# conversation2_df = obj2.query_delta_conversations(namespace2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation1_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj1.query_every_namespace('What was discussed about Atlassian ?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj1.query_delta_conversations_all_namespaces('What was discussed about Atlassian ?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obj1.query_every_namespace(query='little over six months')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = obj1.get_entire_namespace_data(namespace1)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
