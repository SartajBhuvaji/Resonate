{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sbhuv\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pinecone\\data\\index.py:1: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import dotenv\n",
    "import json\n",
    "import datetime\n",
    "import os\n",
    "import uuid\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "import time\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "# PENDING : Move these to a config file\n",
    "INDEX_NAME = 'langchain-retrieval-transcript'\n",
    "PINECONE_VECTOR_DIMENSION = 1536\n",
    "PINECONE_UPSERT_BATCH_LIMIT = 90\n",
    "PINECONE_TOP_K_RESULTS = 2\n",
    "DELTA = 2\n",
    "CLOUD_PROVIDER = 'aws'\n",
    "REGION = 'us-west-2'\n",
    "METRIC = 'cosine'\n",
    "\n",
    "EMBEDDING = 'OpenAI'\n",
    "EMBEDDING_MODEL = 'text-embedding-ada-002'\n",
    "\n",
    "NAMESPACE = 'default_namespace'\n",
    "master_json_file = 'master_meeting_details'\n",
    "\n",
    "class PineconeServerless:\n",
    "    def __init__(self) -> None:\n",
    "        PINECONE_API_KEY = os.getenv('PINECONE_SERVERLESS_API_KEY') or 'PINECONE_SERVERLESS_API_KEY'\n",
    "        self.OPENAI_API_KEY = os.getenv('OPENAI_API_KEY') or 'OPENAI_API_KEY'\n",
    "        self.index_name = INDEX_NAME\n",
    "        self.meeting_title = None\n",
    "        self.pinecone = Pinecone(api_key=PINECONE_API_KEY)\n",
    "        self.base_data_path = os.path.join(os.getcwd(), '../../','bin/data/', NAMESPACE)\n",
    "        self.master_json_file = os.path.join(self.base_data_path, master_json_file)\n",
    "        self.response = None\n",
    "\n",
    "    def check_index_already_exists(self) -> bool:\n",
    "        return self.index_name in self.pinecone.list_indexes()\n",
    "\n",
    "    def _get_vector_embedder(self, EMBEDDING: str = 'OpenAI'):\n",
    "        if EMBEDDING == 'OpenAI':\n",
    "            return OpenAIEmbeddings(\n",
    "                model=EMBEDDING_MODEL,\n",
    "                openai_api_key=self.OPENAI_API_KEY)\n",
    "\n",
    "    def _get_index(self):\n",
    "        return self.pinecone.Index(self.index_name)\n",
    "    \n",
    "    def _create_index(self, INDEX_NAME: str) -> None:\n",
    "        try:\n",
    "            self.pinecone.create_index(\n",
    "                name=INDEX_NAME,\n",
    "                metric=METRIC,\n",
    "                dimension=PINECONE_VECTOR_DIMENSION,\n",
    "            \n",
    "                spec=ServerlessSpec(\n",
    "                    cloud=CLOUD_PROVIDER, \n",
    "                    region=REGION,\n",
    "                    # pod_type=\"p1.x1\",\n",
    "                ) \n",
    "            )    \n",
    "\n",
    "            while not self.pinecone.describe_index(self.index_name).status['ready']:\n",
    "                time.sleep(5)\n",
    "\n",
    "        except Exception as e:\n",
    "            print('Index creation failed: ', e)      \n",
    "\n",
    "\n",
    "    def describe_index_stats(self) -> dict:\n",
    "        try:\n",
    "            index = self._get_index()\n",
    "            return index.describe_index_stats()\n",
    "        except Exception as e:\n",
    "            print('Index does not exist: ', e)\n",
    "            return {}\n",
    "\n",
    "    \n",
    "    def _delete_index(self, index_name: str) -> None:\n",
    "        try:\n",
    "            self.pinecone.delete_index(index_name)\n",
    "        except Exception as e:\n",
    "            print('Index does not exist: ', e)\n",
    "\n",
    "\n",
    "    def _create_master_json(self) -> dict:\n",
    "\n",
    "        data = {\n",
    "                \"index\": INDEX_NAME,\n",
    "                \"namespace\": NAMESPACE,\n",
    "                \"last_conversation_no\": 0,\n",
    "                \"meetings\" :[]\n",
    "        }\n",
    "        #print('master_json_file: ', self.master_json_file)\n",
    "        if not os.path.exists(self.base_data_path):\n",
    "            os.makedirs(self.base_data_path)\n",
    "            \n",
    "        #     with open(self.master_json_file, 'w') as f:\n",
    "        #         json.dump(data, f)\n",
    "        \n",
    "        #with open(master_json_file+'.json', 'r') as f:\n",
    "        #os.path.join(self.base_data_path, f'{master_json_file}.json')\n",
    "\n",
    "        meeting_details_file = os.path.join(self.base_data_path, f'{master_json_file}.json') \n",
    "        print('meeting_details_file: ', meeting_details_file)\n",
    "        #print('master_json_file: ', self.master_json_file   )\n",
    "        with open(meeting_details_file, 'w') as f:\n",
    "            json.dump(data, f, indent=4)\n",
    "\n",
    "\n",
    "    def _update_master_json(self, meeting_uuid:str, meeting_title:str, last_conversation_no:int,\n",
    "                               meeting_members:list[str], meeting_video_file:bool, time_stamp:str ) -> dict:\n",
    "    \n",
    "        meeting_details_file = os.path.join(self.base_data_path, f'{master_json_file}.json')\n",
    "        with open(meeting_details_file, 'r+') as f:\n",
    "            data = json.load(f)\n",
    "            print(\"MASTER JSON: LOADED \", data['last_conversation_no'])\n",
    "\n",
    "            data['last_conversation_no'] = last_conversation_no \n",
    "            data['meetings'].append(\n",
    "                {\n",
    "                        \"meeting_uuid\" : meeting_uuid,\n",
    "                        \"meeting_title\" : meeting_title,\n",
    "                        \"meeting_date\" : time_stamp,\n",
    "                        \"meeting_video_file\" : meeting_video_file,\n",
    "                        \"meeting_members\" : meeting_members,\n",
    "                        \"meeting_summary\" : None\n",
    "                }\n",
    "            )\n",
    "            print(\"UPDATED MASTER JSON: \", data['last_conversation_no'] )\n",
    "            return data\n",
    "               \n",
    "    def _get_meeting_members(self, transcript: pd.DataFrame) -> list[str]:\n",
    "        return list(transcript['speaker_label'].unique())\n",
    "\n",
    "    def _create_new_meeting_json(self, meeting_uuid:str, meeting_title:str, last_conversation_no:int,\n",
    "                                  meeting_members:list[str], meeting_video_file:bool, time_stamp:str) -> dict:\n",
    "        data = {\n",
    "                \"index\": INDEX_NAME,\n",
    "                \"namespace\": NAMESPACE,\n",
    "\n",
    "                \"meeting_title\" : meeting_title,\n",
    "                \"meeting_uuid\" : meeting_uuid,\n",
    "                \"meeting_date\" : time_stamp,\n",
    "\n",
    "                \"last_conversation_no\": last_conversation_no,\n",
    "                \"meeting_video_file\": meeting_video_file,\n",
    "                \"meeting_members\": meeting_members,\n",
    "                \"meeting_summary\" : None,\n",
    "        } \n",
    "\n",
    "        meeting_details_file = os.path.join(self.base_data_path,f'{meeting_uuid}.json') \n",
    "        with open(meeting_details_file, 'w') as f:\n",
    "            json.dump(data, f, indent=4)\n",
    "\n",
    "    def _get_last_conversation_no(self) -> list[str]:   \n",
    "\n",
    "        meeting_details_file = os.path.join(self.base_data_path, f'{master_json_file}.json')\n",
    "        with open(meeting_details_file, 'r') as f:\n",
    "            data = json.load(f)\n",
    "            print('last_conversation_no fetched from master json: ', data['last_conversation_no'])\n",
    "            return data['last_conversation_no']\n",
    "\n",
    "    def _set_new_meeting_json(self, meeting_uuid: str, meeting_title: str, last_conversation_no: str,\n",
    "                               meeting_members: list[str], meeting_video_file: bool) -> dict:\n",
    "        \n",
    "        time_stamp = str(datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\"))\n",
    "        # if not os.path.exists(master_json_file):\n",
    "        #     self._create_master_json()\n",
    "\n",
    "        self._create_new_meeting_json(meeting_uuid, meeting_title, last_conversation_no, meeting_members, meeting_video_file, time_stamp)\n",
    "        data = self._update_master_json(meeting_uuid, meeting_title, last_conversation_no, meeting_members, meeting_video_file, time_stamp)   \n",
    "\n",
    "        meeting_details_file = os.path.join(self.base_data_path, f'{master_json_file}.json')\n",
    "        with open(meeting_details_file, 'w') as f:\n",
    "            json.dump(data, f, indent=4)\n",
    "\n",
    "    def pinecone_upsert(self, transcript: pd.DataFrame, meeting_video_file: bool=False, meeting_title: str = 'Unnamed') -> None:\n",
    "        '''\n",
    "        Upserts the transcript into Pinecone\n",
    "        '''\n",
    "        texts = []\n",
    "        metadatas = []\n",
    "        \n",
    "        last_conversation_no = self._get_last_conversation_no()\n",
    "        print('last_conversation_no: ', last_conversation_no)\n",
    "        last_conversation_no = int(last_conversation_no) #+ 1\n",
    "        \n",
    "        embed = self._get_vector_embedder(EMBEDDING)\n",
    "        meeting_members = self._get_meeting_members(transcript)\n",
    "        meeting_uuid = str(uuid.uuid4())\n",
    "        index = self._get_index()\n",
    "\n",
    "        for _ , record in transcript.iterrows():\n",
    "            metadata = {\n",
    "                'speaker': record['speaker_label'],\n",
    "                'start_time': round(record['start_time'], 4), # fix a time format\n",
    "                # 'meeting_no': meeting_no,\n",
    "                'text': record['text'], \n",
    "                'meeting_uuid': meeting_uuid\n",
    "            }        \n",
    "            texts.append(record['text']) \n",
    "            metadatas.append(metadata)\n",
    "\n",
    "            if len(texts) >= PINECONE_UPSERT_BATCH_LIMIT:\n",
    "                ids = list(map(lambda i: str(i+1), range(last_conversation_no, last_conversation_no + len(texts))))\n",
    "                print('ids: ', ids)\n",
    "                last_conversation_no += len(texts)\n",
    "                embeds = embed.embed_documents(texts)\n",
    "                try:\n",
    "                    index.upsert(vectors=zip(ids, embeds, metadatas), namespace=NAMESPACE)\n",
    "                except Exception as e:\n",
    "                    print('Error upserting into Pinecone: ', e)    \n",
    "                texts = []\n",
    "                metadatas = []\n",
    "\n",
    "        if len(texts) > 0:\n",
    "            ids = list(map(lambda i: str(i+1), range(last_conversation_no, last_conversation_no + len(texts))))\n",
    "            last_conversation_no += len(texts)\n",
    "            print('ids: ', ids)\n",
    "            embeds = embed.embed_documents(texts)\n",
    "            try:\n",
    "                index.upsert(vectors=zip(ids, embeds, metadatas), namespace=NAMESPACE)\n",
    "            except Exception as e:\n",
    "                print('Error upserting into Pinecone: ', e)\n",
    "\n",
    "        print(\"Sending last_conversation_no to update main \" ,last_conversation_no)\n",
    "        self._set_new_meeting_json(meeting_uuid, meeting_title, last_conversation_no, meeting_members, meeting_video_file)  \n",
    "\n",
    "\n",
    "    def _extract_id_from_response(self, response: list) -> list[int]:\n",
    "        return list(int(match['id']) for match in response['matches'])    \n",
    "\n",
    "\n",
    "    def query_pinecone(self, query: str, in_filter: list[str]=[]) -> list:\n",
    "        '''\n",
    "        Queries Pinecone for the given query\n",
    "        '''\n",
    "        try:\n",
    "            index = self._get_index()\n",
    "            embed = self._get_vector_embedder(EMBEDDING)\n",
    "            self.response = index.query(\n",
    "                vector= embed.embed_documents([query])[0],\n",
    "                namespace=NAMESPACE, \n",
    "                top_k=PINECONE_TOP_K_RESULTS,\n",
    "                include_metadata=True,\n",
    "                filter={\"meeting_uuid\": {\"$in\": in_filter}},\n",
    "            )\n",
    "            return self.response\n",
    "        except Exception as e:\n",
    "            print('Error querying Pinecone: ', e)\n",
    "        return []\n",
    "        \n",
    "\n",
    "    def query_delta_conversations(self) -> pd.DataFrame: \n",
    "        '''\n",
    "        Queries Pinecone for the given query and returns the delta conversations\n",
    "        '''\n",
    "        ids = self._extract_id_from_response(self.response)\n",
    "        last_conversation_no = self._get_last_conversation_no()\n",
    "        index = self._get_index()\n",
    "        conversation = {}\n",
    "\n",
    "        for id in ids: \n",
    "            left = id - DELTA if id - DELTA > 0 else 1\n",
    "            right = id + DELTA if id + DELTA <= last_conversation_no else last_conversation_no\n",
    "            window = [str(i) for i in range(left, right + 1)]    \n",
    "            try:\n",
    "                fetch_response = index.fetch(ids=window, namespace=NAMESPACE)\n",
    "                conversation[id] = fetch_response\n",
    "            except Exception as e:\n",
    "                print('Error fetching from Pinecone for id:', id, \"Error:\", e)\n",
    "                continue\n",
    "\n",
    "        print('conversation length: ', len(conversation))\n",
    "        return self._parse_fetch_conversations(conversation)\n",
    "\n",
    "    def _parse_fetch_conversations(self, conversation) -> pd.DataFrame:  \n",
    "        data_rows = []\n",
    "        for primary_hit_id, primary_hit_data in conversation.items():\n",
    "            for _ , vector_data in primary_hit_data['vectors'].items():\n",
    "                id = vector_data['id']\n",
    "                meeting_uuid = vector_data['metadata']['meeting_uuid']\n",
    "                speaker = vector_data['metadata']['speaker']\n",
    "                start_time = vector_data['metadata']['start_time']\n",
    "                text = vector_data['metadata']['text']\n",
    "                \n",
    "                data_rows.append((primary_hit_id, id, meeting_uuid, speaker, start_time, text))\n",
    "\n",
    "        columns = ['primary_id', 'id', 'meeting_uuid', 'speaker', 'start_time', 'text']\n",
    "        delta_conversation_df = pd.DataFrame(data_rows, columns=columns)\n",
    "        delta_conversation_df = delta_conversation_df.sort_values(by=['id'])\n",
    "        print('LENGTH delta_conversation_df: ', len(delta_conversation_df))\n",
    "        delta_conversation_df = delta_conversation_df.drop_duplicates(subset=['id'])\n",
    "        return delta_conversation_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# namespace1 = 'namespace3'\n",
    "obj1 = PineconeServerless()\n",
    "\n",
    "# namespace2 = 'namespace2'\n",
    "# obj2 = PineconeServerless()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obj1._delete_index('langchain-retrieval-transcript')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj1._create_master_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj1._create_index('langchain-retrieval-transcript')\n",
    "# # # ## obj2._create_index(INDEX_NAME)  # No need as both obj1 and obj2 are pointing to the same index\n",
    "\n",
    "print(obj1.describe_index_stats())\n",
    "## print(obj2.describe_index_stats()) # No need as both obj1 and obj2 are pointing to the same index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 3):\n",
    "    print(i)\n",
    "    transcript = pd.read_csv(f'transcript_{i}.csv')\n",
    "    transcript.dropna(inplace=True)\n",
    "    obj1.pinecone_upsert(transcript)\n",
    "    time.sleep(5)\n",
    "\n",
    "# for i in range(3, 5):\n",
    "#     print(i)\n",
    "#     transcript = pd.read_csv(f'transcript_{i}.csv')\n",
    "#     transcript.dropna(inplace=True)\n",
    "#     obj2.pinecone_upsert(transcript)\n",
    "#     time.sleep(5)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 1536,\n",
       " 'index_fullness': 0.0,\n",
       " 'namespaces': {'default_namespace': {'vector_count': 60}},\n",
       " 'total_vector_count': 60}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj1.describe_index_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to be received via clustering model\n",
    "in_filter = ['9e0d7f0b-ff8b-4236-b3d3-71ea6ac84c3b']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sbhuv\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.embeddings.openai.OpenAIEmbeddings` was deprecated in langchain-community 0.1.0 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'matches': [{'id': '43',\n",
      "              'metadata': {'meeting_uuid': '9e0d7f0b-ff8b-4236-b3d3-71ea6ac84c3b',\n",
      "                           'speaker': 'spk_2',\n",
      "                           'start_time': 17.8088,\n",
      "                           'text': 'six, little over six months or'},\n",
      "              'score': 0.938304603,\n",
      "              'values': []},\n",
      "             {'id': '42',\n",
      "              'metadata': {'meeting_uuid': '9e0d7f0b-ff8b-4236-b3d3-71ea6ac84c3b',\n",
      "                           'speaker': 'spk_0',\n",
      "                           'start_time': 17.6677,\n",
      "                           'text': 'Was it within the past year for longer? I '\n",
      "                                   'thought it was  within the past year. I\\r\\n'\n",
      "                                   ' think it was in the last'},\n",
      "              'score': 0.775335848,\n",
      "              'values': []}],\n",
      " 'namespace': 'default_namespace',\n",
      " 'usage': {'read_units': 6}}\n",
      "*************************\n"
     ]
    }
   ],
   "source": [
    "response1 = obj1.query_pinecone('little over six months', in_filter)\n",
    "print(response1)\n",
    "print('*'*25)\n",
    "# response2 = obj2.query_pinecone('What was discussed about Atlassian ?', namespace2)\n",
    "# print(response2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[43, 42]\n"
     ]
    }
   ],
   "source": [
    "print(obj1._extract_id_from_response(response1))\n",
    "# print(obj2._extract_id_from_response(response2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last_conversation_no fetched from master json:  60\n",
      "conversation length:  2\n",
      "LENGTH delta_conversation_df:  10\n"
     ]
    }
   ],
   "source": [
    "conversation1_df = obj1.query_delta_conversations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>primary_id</th>\n",
       "      <th>id</th>\n",
       "      <th>meeting_uuid</th>\n",
       "      <th>speaker</th>\n",
       "      <th>start_time</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>42</td>\n",
       "      <td>40</td>\n",
       "      <td>9e0d7f0b-ff8b-4236-b3d3-71ea6ac84c3b</td>\n",
       "      <td>spk_0</td>\n",
       "      <td>17.2502</td>\n",
       "      <td>Yeah, I, lets, lets go with vulnerability mana...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>43</td>\n",
       "      <td>41</td>\n",
       "      <td>9e0d7f0b-ff8b-4236-b3d3-71ea6ac84c3b</td>\n",
       "      <td>spk_1</td>\n",
       "      <td>17.5293</td>\n",
       "      <td>old. I was, I mean, we did the fuzzing acquisi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>43</td>\n",
       "      <td>42</td>\n",
       "      <td>9e0d7f0b-ff8b-4236-b3d3-71ea6ac84c3b</td>\n",
       "      <td>spk_0</td>\n",
       "      <td>17.6677</td>\n",
       "      <td>Was it within the past year for longer? I thou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>43</td>\n",
       "      <td>43</td>\n",
       "      <td>9e0d7f0b-ff8b-4236-b3d3-71ea6ac84c3b</td>\n",
       "      <td>spk_2</td>\n",
       "      <td>17.8088</td>\n",
       "      <td>six, little over six months or</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>43</td>\n",
       "      <td>44</td>\n",
       "      <td>9e0d7f0b-ff8b-4236-b3d3-71ea6ac84c3b</td>\n",
       "      <td>spk_3</td>\n",
       "      <td>17.8415</td>\n",
       "      <td>so. It was,  it was\\r\\n like last spring or su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>43</td>\n",
       "      <td>45</td>\n",
       "      <td>9e0d7f0b-ff8b-4236-b3d3-71ea6ac84c3b</td>\n",
       "      <td>spk_1</td>\n",
       "      <td>17.9246</td>\n",
       "      <td>did another set of press around the integratio...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   primary_id  id                          meeting_uuid speaker  start_time  \\\n",
       "7          42  40  9e0d7f0b-ff8b-4236-b3d3-71ea6ac84c3b   spk_0     17.2502   \n",
       "4          43  41  9e0d7f0b-ff8b-4236-b3d3-71ea6ac84c3b   spk_1     17.5293   \n",
       "3          43  42  9e0d7f0b-ff8b-4236-b3d3-71ea6ac84c3b   spk_0     17.6677   \n",
       "0          43  43  9e0d7f0b-ff8b-4236-b3d3-71ea6ac84c3b   spk_2     17.8088   \n",
       "1          43  44  9e0d7f0b-ff8b-4236-b3d3-71ea6ac84c3b   spk_3     17.8415   \n",
       "2          43  45  9e0d7f0b-ff8b-4236-b3d3-71ea6ac84c3b   spk_1     17.9246   \n",
       "\n",
       "                                                text  \n",
       "7  Yeah, I, lets, lets go with vulnerability mana...  \n",
       "4  old. I was, I mean, we did the fuzzing acquisi...  \n",
       "3  Was it within the past year for longer? I thou...  \n",
       "0                     six, little over six months or  \n",
       "1  so. It was,  it was\\r\\n like last spring or su...  \n",
       "2  did another set of press around the integratio...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation1_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obj1.query_every_namespace(query='little over six months')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
