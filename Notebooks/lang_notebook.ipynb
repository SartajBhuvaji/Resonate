{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import dotenv\n",
    "dotenv.load_dotenv()\n",
    "import os\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY') or 'OPENAI_API_KEY'\n",
    "PINECONE_API_KEY = os.getenv('PINECONE_SERVERLESS_API_KEY') or 'PINECONE_SERVERLESS_API_KEY'\n",
    "index_name = 'langchain-retrieval-transcript'\n",
    "\n",
    "namespace = 'langchain'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'indexes': [{'dimension': 1536,\n",
      "              'host': 'langchain-retrieval-transcript-kp69ciw.svc.apw5-4e34-81fa.pinecone.io',\n",
      "              'metric': 'cosine',\n",
      "              'name': 'langchain-retrieval-transcript',\n",
      "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-west-2'}},\n",
      "              'status': {'ready': True, 'state': 'Ready'}}]}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'dimension': 1536,\n",
       " 'index_fullness': 0.0,\n",
       " 'namespaces': {'langchain': {'vector_count': 144}},\n",
       " 'total_vector_count': 144}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pinecone = Pinecone(api_key=PINECONE_API_KEY)\n",
    "print(pinecone.list_indexes())\n",
    "\n",
    "index = pinecone.Index(index_name)\n",
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install -U langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Pinecone\n",
    "from langchain.schema import SystemMessage, HumanMessage, AIMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# embedding model\n",
    "model_name = 'text-embedding-ada-002'\n",
    "embed = OpenAIEmbeddings(\n",
    "    model=model_name,\n",
    "    openai_api_key=OPENAI_API_KEY\n",
    ")\n",
    "\n",
    "# pinecone store\n",
    "text_field = 'text' # the field in the metadata that contains the text and would be used for retrieval\n",
    "vector_store = Pinecone(\n",
    "    index, embed, text_field, namespace=namespace)\n",
    "\n",
    "# chat model\n",
    "llm = ChatOpenAI(\n",
    "\ttemperature=0,\n",
    "\topenai_api_key=OPENAI_API_KEY,\n",
    "\tmodel_name=\"gpt-3.5-turbo\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using BasePromptTemplate  \n",
    "\n",
    "from langchain_core.prompts.base import BasePromptTemplate\n",
    "\n",
    "# define the prompt template\n",
    "prompt_template = BasePromptTemplate(\n",
    "    system_prompt=\"System: {system_message}\\nHuman: {human_message}\\n\",\n",
    "    human_prompt=\"System: {system_message}\\nHuman: {human_message}\\n\",\n",
    "    ai_prompt=\"System: {system_message}\\nHuman: {human_message}\\nAI: {ai_message}\\n\",\n",
    ")\n",
    "\n",
    "# define the schema\n",
    "schema = {\n",
    "    \"system_message\": SystemMessage,\n",
    "    \"human_message\": HumanMessage,\n",
    "    \"ai_message\": AIMessage,\n",
    "}\n",
    "\n",
    "# define the context\n",
    "context = {\n",
    "    \"system_message\": \"Hello, I am a bot.\",\n",
    "    \"human_message\": \"Hello, I am a human.\",\n",
    "    \"ai_message\": \"Hello, I am an AI.\",\n",
    "}\n",
    "\n",
    "# adding context to the prompt template\n",
    "prompt_template.add_context(context)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.conversation.memory import ConversationBufferWindowMemory\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "\n",
    "template = \"\"\"Answer the question based on the context below. If the\n",
    "question cannot be answered using the information provided answer\n",
    "with \"I don't know\".\n",
    "\n",
    "Current conversation: {history}\n",
    "\n",
    "Context: {context}\t\n",
    "\n",
    "Question: {input}\n",
    "Answer: \"\"\"\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=['history','context','input'],\n",
    "    template=template\n",
    ")\n",
    "\n",
    "# conversation chain\n",
    "conversation = ConversationChain(\n",
    "\tllm=llm,\n",
    "\tprompt = prompt_template,\n",
    "\tmemory=ConversationBufferWindowMemory(k=10),\n",
    "\t\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "{history}\n",
      "Human: {input}\n",
      "AI:\n"
     ]
    }
   ],
   "source": [
    "print(conversation.prompt.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
