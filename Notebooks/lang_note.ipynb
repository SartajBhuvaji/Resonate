{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sbhuv\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pinecone\\data\\index.py:1: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import dotenv\n",
    "dotenv.load_dotenv()\n",
    "import os\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY') or 'OPENAI_API_KEY'\n",
    "PINECONE_API_KEY = os.getenv('PINECONE_SERVERLESS_API_KEY') or 'PINECONE_SERVERLESS_API_KEY'\n",
    "index_name = 'langchain-retrieval-transcript'\n",
    "\n",
    "namespace = 'langchain'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'indexes': [{'dimension': 1536,\n",
      "              'host': 'langchain-retrieval-transcript-kp69ciw.svc.apw5-4e34-81fa.pinecone.io',\n",
      "              'metric': 'cosine',\n",
      "              'name': 'langchain-retrieval-transcript',\n",
      "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-west-2'}},\n",
      "              'status': {'ready': True, 'state': 'Ready'}}]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sbhuv\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.embeddings.openai.OpenAIEmbeddings` was deprecated in langchain-community 0.1.0 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "pinecone = Pinecone(api_key=PINECONE_API_KEY)\n",
    "print(pinecone.list_indexes())\n",
    "\n",
    "model_name = 'text-embedding-ada-002'\n",
    "\n",
    "embed = OpenAIEmbeddings(\n",
    "    model=model_name,\n",
    "    openai_api_key=OPENAI_API_KEY\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create index\n",
    "if index_name not in pinecone.list_indexes():\n",
    "    pinecone.create_index(\n",
    "        name=index_name,\n",
    "        metric='cosine',\n",
    "        dimension=1536,  # model_name = 'text-embedding-ada-002'; 1536 dim of text-embedding-ada-002\n",
    "        \n",
    "        spec=ServerlessSpec(\n",
    "        cloud='aws', \n",
    "        region='us-west-2'\n",
    "        # pod_type=\"p1.x1\",\n",
    "        ) \n",
    "    )   \n",
    "    \n",
    "while not pinecone.describe_index(index_name).status['ready']:\n",
    "    time.sleep(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 1536,\n",
       " 'index_fullness': 0.0,\n",
       " 'namespaces': {'langchain': {'vector_count': 144}},\n",
       " 'total_vector_count': 144}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = pinecone.Index(index_name)\n",
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript = pd.read_csv('aws_parsed_transcript.csv')\n",
    "print(transcript.shape)\n",
    "transcript.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from uuid import uuid4\n",
    "\n",
    "batch_limit = 90\n",
    "texts = []\n",
    "metadatas = []\n",
    "meeting_id = 1\n",
    "start_id = 0\n",
    "\n",
    "for i, record in tqdm(transcript.iterrows()):\n",
    "    # first get metadata fields for this record\n",
    "    metadata = {\n",
    "        'speaker': record['speaker_label'],\n",
    "        'start_time': round(record['start_time'], 4), # limit to 4 decimal places \n",
    "        'meeting_id': meeting_id,\n",
    "        'text': record['text'], # Storing the text in the metadata for now, later we'd need to decode it from vectors\n",
    "    }\n",
    "\n",
    "    record_texts = record['text']\n",
    "\n",
    "    texts.append(record_texts)\n",
    "    metadatas.append(metadata)\n",
    "\n",
    "    # print(texts)\n",
    "    # print(metadatas)\n",
    "\n",
    "    # if we've reached the batch limit, then index the batch\n",
    "    if len(texts) >= batch_limit:\n",
    "        #ids = [str(uuid4()) for _ in range(len(texts))]\n",
    "        ids = [str(i+1) for i in range(start_id,(start_id + len(texts)))]\n",
    "        start_id += len(texts)\n",
    "        embeds = embed.embed_documents(texts)\n",
    "        index.upsert(vectors=zip(ids, embeds, metadatas), namespace=namespace)\n",
    "        texts = []\n",
    "        metadatas = []\n",
    "        meeting_id += 1\n",
    "\n",
    "# add any remaining texts to the index\n",
    "if len(texts) > 0:\n",
    "    #ids = [str(uuid4()) for _ in range(len(texts))]\n",
    "    ids = [str(i+1) for i in range(start_id,(start_id + len(texts)))]\n",
    "    embeds = embed.embed_documents(texts)\n",
    "    index.upsert(vectors=zip(ids, embeds, metadatas), namespace=namespace)\n",
    "    \n",
    "time.sleep(5)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 1536,\n",
       " 'index_fullness': 0.0,\n",
       " 'namespaces': {'langchain': {'vector_count': 144}},\n",
       " 'total_vector_count': 144}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time.sleep(10)\n",
    "index.describe_index_stats()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sbhuv\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_community\\vectorstores\\pinecone.py:75: UserWarning: Passing in `embedding` as a Callable is deprecated. Please pass in an Embeddings object instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import Pinecone\n",
    "from langchain.schema import SystemMessage, HumanMessage, AIMessage\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "text_field = 'text' # the field in the metadata that contains the text and would be used for retrieval\n",
    "vector_store = Pinecone(\n",
    "    index, embed.embed_query, text_field, namespace=namespace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Aside, aside from the goofy name, I thought that, um, and of course we cant talk about any of it on youtube. But man, that was like a really awesome look back at like heres some wins and heres some exciting things that weve done. Um And so I think we just need to get better at this as a company. I think this exercise is an opportunity that um I think we do a little bit of that, but this is doing it more. So, uh Sonia, you have some questions here for elitists.', metadata={'meeting_id': 1.0, 'speaker': 'spk_0', 'start_time': 22.5338}),\n",
       " Document(page_content='to come back and we talk about it on Monday in our little social.', metadata={'meeting_id': 2.0, 'speaker': 'spk_3', 'start_time': 41.7078}),\n",
       " Document(page_content='uh cool. Well, were almost up on time but I did wanna share one thing that kind of came in like basically Friday, it was like late Thursday for me. So this uh this is another thing, a lot, a lot of these things, I dont have like a ton of context on. Um I just know like were just trying to up our game. So like, hey, lets move fast and do the best we can do. Um So this is a little bit of a messaging framework and then the assignment that was given to me was to, to fill in these boxes here. So, uh and then this was the fodder that I got. So there was these ones that said like from roadmap to company vision, we are transparent and a single source of truth. Countless possibilities. Honestly, I think this ones really like, really pithy and catchy. Uh I really like it. Um I really liked uh whats the other one that I like? But not everyone does um all in one for everyone. So I just uh I dont know, I like, I just think so theres something catchy about that. Um But I know that kind of catchy is not everyones cup of tea. All that to say is um the assignment here was that I took, it was to write a pithy, few words like punchy statement for each of these three things. Uh So I ended up with a single source of truth. Countless possibilities. I like that a lot. Um This one I went with end to end control over your software factory, not as pithy but or like not as catchy, but um I like it.', metadata={'meeting_id': 2.0, 'speaker': 'spk_0', 'start_time': 33.2587}),\n",
       " Document(page_content='So, so this is exactly where this is headed to um is the idea that um during the product keynote and then ideally during Kid Sids keynote as well that theres some level of like an announcement that we make that. Then the press cares about that announcement. Um Again, really tough to do at cl because its all been around for a while. So this is this is our attempt to head to that area is to say like these are the announcements of the event and yeah, maybe its some things that have been around for a while, but like now theyre at a level of maturity now, you know, theres, theres a reason to be excited about this. So thats kind of where its coming from. So I put uh a top five overall. Maybe we can just, uh you know,', metadata={'meeting_id': 1.0, 'speaker': 'spk_0', 'start_time': 14.8373}),\n",
       " Document(page_content='What about like a New and Christies uh product keynote at commit?  Are we that tailing this with\\r\\n that?', metadata={'meeting_id': 1.0, 'speaker': 'spk_5', 'start_time': 14.7357})]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = 'I want to talk about the future of the company'\n",
    "vector_store.similarity_search(query, k=5, namespace=namespace)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_prompt(query: str):\n",
    "    # get top 3 results from knowledge base\n",
    "    results = vector_store.similarity_search(query, k=3, namespace=namespace)\n",
    "    source_knowledge = \"\\n\".join([x.page_content for x in results])\n",
    "    \n",
    "    augmented_prompt = f\"\"\"Using the contexts below, answer the query in as much detail as possible.\n",
    "\n",
    "    Contexts:\n",
    "    {source_knowledge}\n",
    "\n",
    "    Query: {query}\"\"\"\n",
    "    return augmented_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the contexts below, answer the query in as much detail as possible.\n",
      "\n",
      "    Contexts:\n",
      "    the door for NASCAR Partnership. Hey NASCAR goes fast and turns left because it get loud.\n",
      "Its something Ricky Bobby would have said if anyone hasnt seen Talladega Nights,  they should, thats\n",
      " the homework for this weekend\n",
      "more speed. The Ricky Bobby and me really did like the no trade offs there. Parker.\n",
      "\n",
      "    Query: What was discussed about NASCAR?\n"
     ]
    }
   ],
   "source": [
    "print(augment_prompt(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From the given contexts, it appears that there was a discussion about NASCAR and its partnership. The mention of NASCAR going fast and turning left suggests the nature of NASCAR racing. It is also mentioned that NASCAR is loud. There is a reference to the movie \"Talladega Nights\" and the character Ricky Bobby, who is known for his love of NASCAR. The mention of \"no trade offs\" implies that there is a positive opinion or sentiment towards NASCAR and its qualities. Overall, the discussion seems to revolve around the excitement, speed, and enjoyment associated with NASCAR racing.\n"
     ]
    }
   ],
   "source": [
    "query  = 'What was discussed about NASCAR?'\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant.\"),\n",
    "    HumanMessage(content=\"Hi AI, how are you today?\"),\n",
    "    AIMessage(content=\"I'm great thank you. How can I help you?\"),\n",
    "]\n",
    "\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    openai_api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "    model='gpt-3.5-turbo'\n",
    ")\n",
    "\n",
    "# create a new user prompt\n",
    "prompt = HumanMessage(\n",
    "    content=augment_prompt(query)\n",
    ")\n",
    "# add to messages\n",
    "messages.append(prompt)\n",
    "\n",
    "res = chat(messages)\n",
    "\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='the door for NASCAR Partnership. Hey NASCAR goes fast and turns left because it get loud.', metadata={'meeting_id': 2.0, 'speaker': 'spk_3', 'start_time': 39.9427}),\n",
       " Document(page_content='Its something Ricky Bobby would have said if anyone hasnt seen Talladega Nights,  they should, thats\\r\\n the homework for this weekend', metadata={'meeting_id': 2.0, 'speaker': 'spk_4', 'start_time': 41.6003}),\n",
       " Document(page_content='more speed. The Ricky Bobby and me really did like the no trade offs there. Parker.', metadata={'meeting_id': 2.0, 'speaker': 'spk_4', 'start_time': 41.4151}),\n",
       " Document(page_content='What about like a New and Christies uh product keynote at commit?  Are we that tailing this with\\r\\n that?', metadata={'meeting_id': 1.0, 'speaker': 'spk_5', 'start_time': 14.7357}),\n",
       " Document(page_content='Thats where we were last I heard.', metadata={'meeting_id': 1.0, 'speaker': 'spk_3', 'start_time': 3.0255})]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store.similarity_search(query, k=5, namespace=namespace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
